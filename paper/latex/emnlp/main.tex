%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
%\usepackage[hyperref]{emnlp2020}
\usepackage{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsmath,amsfonts}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{siunitx}
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\usepackage{etoolbox}
\makeatletter
\patchcmd\@combinedblfloats{\box\@outputbox}{\unvbox\@outputbox}{}{\errmessage{\noexpand patch failed}}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% latex functions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\ltwo}[1]{\lVert{#1}\rVert}
\newcommand{\indicator}[1]{\mathbbm{1}\!\left[{#1}\right]}

\newcommand{\R}{\mathbb R}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\FindMostSimilar}{\texttt{FindMostSimilar}}
\DeclareMathOperator{\OddOneOut}{\texttt{OddOneOut}}
\DeclareMathOperator{\topk}{\texttt{Topk}}
\DeclareMathOperator{\analogy}{\texttt{Analogy}}

\newcommand{\fixme}[1]{{\color{red}\itshape \textbf{FIXME:} {#1}}}

% what were these here for?
%\newcommand\BibTeX{B\textsc{ib}\TeX}
%\special{papersize=210mm,297mm}

% Paper
\title{Instructions for EMNLP 2020 Proceedings}

\author{Nathan Stringham\\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Mike Izbicki \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
The analogy task introduced by \citet{mikolov2013efficient} has become the standard method to evaluate word embeddings because it correlates well with performance on downstream tasks an
The analogy task is not suitable for low-resource languages,
however, because good performance requires a large amount of training data and meaningful analogies can be difficult to create.
As a result, researchers in working in low-resource settings have created many ad-hoc methods for 

 %however, evidence suggests that analogies may not perform equally well across training contexts.
 %Specifically, this method has proven non-trivial to adapt to languages beyond English and empirical evidence suggests that it requires large amounts of training data.
 As a result, those working in low resource or specialized settings have lacked a standard method to evaluate the quality of word embeddings.
 To address this problem, we propose two different intrinsic evaluation methods, which achieve greater performance sensitivity than the analogy task in low resource settings.
  We demonstrate the flexibility of these methods by using them to tune embeddings for $18$ low resource languages as well as the full list of unicode emojis.
   Furthermore, our methods are designed such that custom test sets can be developed semi-automatically in over $400$ different languages, which helps increase access for research in areas previously inhibited by low amounts of data.
\end{abstract}


%%% Copied this from other document
\section{Introduction}
\label{sec:intro}

%Word embeddings are important in modern NLP models.
Word vector embeddings are widely used in Natural Language Processing (NLP),
and have become a critical tool for achieving state-of-the-art performance in many tasks \cite{}.

\citet{mikolov2013efficient} showed that the word2vec model produces embeddings with useful linear structure, and this structure can be used to solve analogy tasks.
\begin{equation}
\texttt{king} - \texttt{man} + \texttt{woman} \approx \texttt{queen}
.
\end{equation}
To evaluate the quality of their word embeddings,
\citet{mikolov2013efficient} created a $14$ category evaluation set commonly called the Google Analogy Test Set. 
Importantly, good performance on this analogy test set correlates with good performance on downstream tasks like sentiment analysis \cite{}, part of speech tagging \cite{}, and named entity recognition \cite{}.
Improvements to the standard word2vec algorithm such as FastText \cite{bojanowski2016enriching} and GloVe \cite{pennington2014glove} have demonstrated their efficacy by demonstrating improvements on the Google Analogy Test Set.

At the same time, only small consideration has been given to areas where the analogy task either cannot be applied or fails to capture meaningful differences in quality between sets of embeddings.

Bias-variance tradeoff in dimensionality of word embeddings \cite{yin2018dimensionality}

\subsection{Weaknesses of Google Analogy Test Set}

People have talked about the weaknesses of the google analogy set, ie unbalanced between semantic and syntactic categories, mostly syntactic, when we usually care more about semantics.
\cite{}

\subsection{Low-resource word embeddings}

The analogy task is not suitable for low-resource languages,
however, because good performance requires a large amount of training data and meaningful analogies can be difficult to create.
As a result, researchers in working in low-resource settings have created many ad-hoc methods for 

English can be a low resource setting when studied diachronically \cite[e.g.][]{hamilton2016diachronic,hamilton2016cultural,kutuzov2018diachronic,kozlowski2019geometry,dubossarsky2017outta,tang2018state,szymanski2017temporal,liang2018dynamic,chen2017understanding}.

\citet{azarbonyad2017words} considers splits over different political ideologies.

\cite{kulkarni2015statistically} similar.
Along this line of work, \citet{gonen2020simple} is the most similar to ours.
They propose a new method for comparing the similarity of two word embeddings in order to find words that are used differently.
Their method assumes that quality word embeddings have already been trained,
and our evaluation metrics provide a way to ensure that this training is done well.

\citet{NIPS2013_5165} improves the word2vec skipgram model with noise contrastive estimation (NCE).
They report requiring 10x less computation and 4x less training data to get the same results.
\citet{gupta-etal-2019-improving} introduce an extension to the skipgram and fasttext models that uses kernel PCA to reduce the amount of data needed during the training procedure.
(i.e., their method requires less data to train good models.)
They still use the

Due to Zipf's law, some words will be very infrequent.
Analogies are known to now work well on these infrequently used words.

\subsection{Highly multilingual word embeddings}
\citet{al2013polyglot} trained word2vec embeddings on 100 different languages using Wikipedia as the training data.
\citet{grave2018learning} extended this work by training FastText embeddings on 157 languages using data from the Common Crawl project.
\begin{enumerate}
\item
For both papers, the Hindi language had the smallest amount of training data,
with 23 million and 1.8 billion tokens, respectively.
In Section \ref{sec:}, we consider languages with significantly less training data.
For example, the Ancient Hindi language contains only 0.6 million tokens,
and we are still able to generate meaningful word embeddings using our evaluation metrics.

\item
Due to the difficulty of evaluating so many languages, however, the authors evaluate the embeddings only on 10 languages,
and the other 147 remain unevaluated.

\end{enumerate}

%We need high quality word embeddings.
%Caveats: 
%- analogies were evaluated on english dataset only
%- word embeddings were trained in a resource rich scenario
%Still, analogies have become the go-to instrinsic evaluation method.
%- Many people who are trying to make good word embeddings in other languages have simply translated the google analogy set into their chosen language and then added a couple of new categories specific to their language. 
%- 3 CosMul and other attempts to modify the evaluation task...still buying in to the premise
%
%But, none of these approaches have really addressed the issue of not having enough data and difficulty of adapting the analogy task.
%

% other stuff

\begin{figure}
\centering
\includegraphics[width=1\columnwidth]{size_vs_acc.pdf} 
\caption{The plot above compares the regions of effectiveness for our evaluation metrics. 
The analogy task fails to measure change in accuracy of the embeddings until the number of unique words in the training dataset reaches  $2^{16}$, much later than both $\OddOneOut$ and $\topk$. 
Though $\OddOneOut$ seems the clear victor of these methods, experimentation in \ref{sec:experiments} shows that $\topk$ works better in some circumstances.}
\label{fig:size_vs_acc}
\end{figure}


Most work regarding word embeddings has been carried out in resource rich settings. 
In particular, the original word2vec embeddings were trained on a GoogleNews corpus containing 6 billion English tokens of which 783 million were unique \cite{mikolov2013efficient}.
 Unsurprisingly, greater amounts of training data leads to more accurate embeddings.
  But in reality, there are many domains in which this amount of data is impractical or even impossible to obtain. 
  Consider for example, a low resource language such as Telugu or a historical language like Latin.
  Furthermore, resource rich languages can quickly become low resource when considered from a specialized perspective.
   Those studying the evolution of language over time often require very specific corpora which in turn restricts the quantity of data. 

More recently, embeddings have even been generated for tokens that serve the purpose of words, but are not considered words in the traditional sense.
 The clearest example of these non-traditional embeddings is emoji, which have emerged as an important feature of natural language data in social media.
  In this setting it is unclear whether the analogy task should be applied and if so how to do so systematically and at scale.

In this paper, we demonstrate that the analogy task is not always suitable for evaluating quality of word embeddings,
 particularly when training resources are limited and when working in niche or specialized domains. 
 As a solution to this problem, we propose $\OddOneOut$ and $\topk$, two alternative evaluation methods which provide greater usability in low resource settings and are more general in nature.
  To further increase flexibility in deployment of embedding evaluation systems we also present Wikidata as a tool for generating test sets for these methods in up to $461$ languages. 


%\section{Contributions}
%\label{sec:contributions}
%\begin{itemize}
%	\item We show that the analogy task introduced by Mikolov et. al is not always suitable for evaluating the quality of word embeddings, particularly when embeddings are trained on small amounts of data.
%	\item We introduce two new methods for evaluating embeddings, $\OddOneOut$ and $\topk$.
%	\begin{itemize}
%		\item Use $\OddOneOut$ and $\topk$ to train emoji2vec model achieving comparable results to \citep{Eisner et al.}
%		\item These methods are more general (works for emojis and word embeddings)
%	\end{itemize}
%	\item We use wikidata to automatically generate test sets in multiple languages
%\end{itemize}

The remainder of the paper is organized as follows.
Section \ref{sec:method} introduces the $\topk$ and $\OddOneOut$ tasks.
Section \ref{sec:experiments}
Section \ref{sec:mca}

\section{Evaluation Methods}
\label{sec:method}
From a technical standpoint, our methods are quite simple and designed to be as generally applicable as possible.
 Both $\OddOneOut$ and $\topk$ require a test set which consists of groups of categories where each category contains words that are semantically similar. 
 The $\OddOneOut$ method seeks to identify the word that does not belong when presented with a group of words.
  For a given word, $\topk$ finds the k most similar words according to the model and compares them to words that belong to the semantic category from which the word was chosen.


We now formalize the methods. 
Assume that there are $m$ categories, that each category has $n$ words, that there are $v$ total words in the vocabulary, and that the words are embedded into $\R^d$.
Let $c_{i,j}$ be the $j$th word in category $i$,
and let $C_i = \{c_{i,1}, c_{i,2}, ..., c_{i,n}\}$ be the $i$th category.

\subsection{The $\topk$ method}
Let $\FindMostSimilar(k,w)$ return the $k$ most similar words in the vocabulary to $w$.
We define the $\topk$ score for class $i$ to be
\begin{equation}
    \topk(k,i) = \frac{1}{n}\sum_{j=1}^n \frac{1}{k}\sum_{\mathrlap{x \in \FindMostSimilar(k, c_{i,j})}} \indicator{x \in C_i}
\end{equation}
and the $\topk$ score for the entire evaluation set to be
\begin{equation}
    \topk(k) = \frac{1}{m}\sum_{i=1}^m \topk(k,i)
    .
\end{equation}

The runtime of $\FindMostSimilar$ is $O(dvk)$.\footnote{
    We use gensim's implementation of $\FindMostSimilar$,
    which uses the naive loop strategy for computing the nearest neighbor.
    Data structures like the $k$d-tree or cover tree could potentially be used to speed up this search,
    but we did not find such data structures necessary.
}
So the runtime of $\topk(k,i)$ is $O(dnk^2v)$
and the runtime of $\topk(k)$ is $O(dnmk^2v)$.
Typically $k$ is small (we recommend $k=3$ in our experiments),
and so the runtime is linear in all of the interesting parameters.
In particular, it is linear in both the size of our input data and evaluation set.

\subsection{The $\OddOneOut$ method}
Define the $\OddOneOut$ score of a set $S$ with $k$ words with respect to a word $w\not\in S$ as

%\begin{equation}
%    \OddOneOut(S,w) = \indicator{w = \hat w},
%\end{equation}
%where
%\begin{equation}
%    \hat w = \argmax_{x \in S\cup\{w\}} \ltwo{x - \mu}
%    \quad
%    \text{and}
%    \quad
%    \mu = \frac1{k+1}\bigg(w + \sum_{i=1}^k{s_i}\bigg)
%    .
%\end{equation}

\begin{equation}
    \OddOneOut(S,w) = \indicator{w = \hat w},
\end{equation}
where
\begin{equation}
    \hat w = \argmax_{x \in S\cup\{w\}} \ltwo{x - \mu}
\end{equation}
\begin{equation}
    %\quad
    \text{and}
    \quad
    \mu = \frac1{k+1}\bigg(w + \sum_{i=1}^k{s_i}\bigg)
    .
\end{equation}


We define the $k$th order $\OddOneOut$ score of a category $i$ to be
\begin{equation}
    \OddOneOut(k,i) = \frac 1 {|P|} \sum_{\mathclap{(S,w) \in P}} \OddOneOut(S,w)
\end{equation}
where
\begin{equation}
    P = \{ (S, w) : S \text{~is a combination of $k$ words from $C_i$}, \text{and~} w \in V-C_i \}
    .
    \label{eq:P}
\end{equation}
In Equation \eqref{eq:P} above,
the total number of values that $S$ can take is ${n \choose k} = O(n^k)$,
and the total number of values that $w$ can take is $O(v)$,
so $|P| = O(n^kv)$.
Finally, we define the $k$-th order $\OddOneOut$ score of the entire evaluation set to be
\begin{equation}
    \OddOneOut(k) = \frac 1 m \sum_{i=1}^m \OddOneOut(k,i)
    .
\end{equation}

The runtime of $\OddOneOut(S,w)$ is $O(dk)$.
So the runtime of $\OddOneOut(k,i)$ is $O(dkn^kv)$ and the runtime of $\OddOneOut(k)$ is $O(dkmn^kv)$.

Comparing the runtimes of $\OddOneOut(k)$ and $\topk(k)$, we can see that $\OddOneOut(k)$ is a factor of $O(m^{k-1}k^{-1})$ slower.
In real world applications, $m >\!\!> k$, and so $\OddOneOut$ will take considerably more time to compute.
In particular, the Mikolov test evaluations in Section \ref{sec:experiments} below use $m=50$ and $k=3$,
so the $\OddOneOut(k)$ score takes approximately 800x longer to compute.

\section{Experiments}
\label{sec:experiments}

We demonstrate the usefulness of our evaluation metrics with two experiments.
First, we show that the $\OddOneOut$ and $\topk$ metrics are better measures of word embedding quality than the analogy metric in the low-resource regime.
Second, we show that the $\OddOneOut$ and $\topk$ metrics are useful for model selection in an emoji embedding task where the analogy task cannot be applied.
This experiment also demonstrates that the $\OddOneOut$ and $\topk$ metrics correlate with downstream task performance.

\subsection{Small dataset word embeddings}

This experiment measures the performance of the $\OddOneOut$, $\topk$, and $\analogy$ metrics as a function of data set size.
\fixme{Figure \ref{fig:size_vs_acc} shows that the $\OddOneOut$ and $\topk$ metrics are $\analogy$.}

%Figure \ref{fig:data_vs_methods} shows the results of the evaluation of these models on three intrinsic tasks-- \texttt{Analogy}, \texttt{OddOneOut}, and \texttt{Topk}.
 %We see that for the various tasks both \texttt{OddOneOut} and \texttt{Topk} start to see an increase in performance with roughly $32x$ less data compared to \texttt{Analogy}. 
 %In addition, though the shapes are roughly similar, the curve for \texttt{OddOneOut} and \texttt{Topk} have steeper slopes before plateauing, 
 %indicating a greater sensitivity to amount of data.
  %Overall, these findings suggest that in the low resource scenarios, the analogy task does will not always provide adequate intrinsic evaluation for word embeddings,
  %especially compared to these simpler methods.

For training data, we use a 2017 dump of the English-language Wikipedia 
that contains 2 billion total tokens and 2 million unique tokens.
The dataset is freely distributed with the popular gensim library \cite{rehurek_lrec} for training word embeddings,
and it is therefore widely used.
State-of-the-art embeddings are trained on significantly larger datasets---%
for example, datasets based on the common crawl contain hundreds of billions of tokens even for non-English languages \cite{buck2014n,grave2018learning}---%
but since our emphasis is on the low-resource setting,
this 2 billion token dataset is sufficient.

Using the wikipedia dataset, we generate a series of synthetic low-resource datasets of varying size.
First, we sort the articles in the wikipedia dataset randomly.%
\footnote{
    This sorting is required for our low-resource datasets to be representative of English language text.
    Without this random sorting step,
    most of our datasets would be based only on articles that begin with the letter \texttt{A},
    and therefore would not contain a representative sample of English words.
 %This is a problem because larger models would have the benefit of both greater amounts of data as well as a greater contextual variety of data compared to smaller ones. 
 %To mitigate this bias we use a linear congruential generator to randomly stream the training data in a way that is reproducible. 
 %Thus, in addition to mitigating model bias tied to the content of training data, our random generator also ensures that each model uses a subset of the training data of models larger than it.
}
Then, each dataset $i$ contains the first $2^i$ tokens in the randomly ordered wikipedia dump.

On each of these low-resource datasets,
we train a word2vec skipgram model with gensim's default hyperparameters\footnote{
Embedding dimension 100, number of epochs 1, learning rate 0.025, window size is 5, min count is 5.
\fixme{
    should we defined min count?
}
},
which are known to work well in many contexts.
Importantly, we do not tune these hyperparameters for each low-resource dataset.
Instead, we use the same hyperparameters because our goal is to isolate the effects of dataset size on the three evaluation metrics.

\fixme{
In order to apply the evaluation metrics,
we need test sets for each metric.
We cannot use the exact same test set because the $\analogy$ metric requires a test set with different formatting than the $\OddOneOut$ and $\topk$ metrics.
In the $\analogy$ task, we are given a set of tuples of the form
\begin{equation*}
    cat1a,cat1b,cat2a,cat2b
\end{equation*}
where the
%To calculate accuracies we use the Google analogy test set to which is easily adapted for use with $\OddOneOut$ and $\topk$ by splitting each relationship pair into two separate categories.

To ensure a fair comparison,
we use the Google Analogy Test Set on the $\analogy$ directly, 
and construct a modified version of this dataset that can be used for the $\OddOneOut$ and $\topk$ metrics.

We use the Google Analogy Test set.
%To ensure fairness in evaluation, we use the 
The format of the test sets for the $\analogy$ task is different than the format for the $\topk$ and $\OddOneOut$ tasks.
The $\analogy$ task test set is forma
We speculate that because the analogy task is measuring this extra information,
it requires more training data.
}

\begin{figure*}
\centering
    \resizebox{6in}{3in}{\includegraphics{cross_cat_acc.pdf} }
\caption{
    The methods perform better on some categories than others. $\topk$  seems to excel in categories that are more homogenous like 'family female',
 while analogies seem to work best with geographical relationships.
  Note that in adapting the Google analogy set to work with our methods required splitting each relationship pair into two separate categories. 
  As a result the analogy score for a given relationship is shown twice;
   one bar in each of the categories that make up the pair.
   }
\label{fig:data_vs_methods}
\end{figure*}


\subsection{Emoji Experiments}
\begin{figure}
\centering
    \resizebox{\columnwidth}{!}{\includegraphics{emoji_exp.pdf} }
\caption{This figures shows the tuning of emoji embeddings across different learning learning rates where the max accuracy for each metric is marked with a point.
 Both $\topk$ and $\OddOneOut$ follow the shape of \cite{eisner2016emoji2vec} area under the curve and accuracy metrics.
 Our methods lead us to select essentially the same hyperparameters as \cite{eisner2016emoji2vec} and reproduced results on the downstream task.}
\label{fig:emoji}
\end{figure}

This second experiment demonstrates the versatility of our methods by applying them to the domain of emoji embeddings.

Emoji embeddings are an important topic of study because they are used to improve the performance of sentiment analysis systems \cite[e.g.]{felbo2017using,barbieri2017emojis,ai2017untangling,wijeratne2017semantics,al2019smile,eisner2016emoji2vec}. 
Unfortunately,
the standard analogy task is not suitable for evaluating the quality of emoji embeddings for two reasons.
First, emoji embeddings are inherently low-resource---%
only 3000 unique emojis exist in the Unicode standard---%
and thus evaluation techniques specifically designed for the low-resource setting will be more effective.
Second, the semantics of most emojis do not allow them to be used in any analogy task.
In particular, the original emoji2vec paper \cite{eisner2016emoji2vec} identifies only \fixme{5} possible emoji analogies.
In order to tune their emoji embeddings, \citet{eisner2016emoji2vec} therefore do not use the analogy task,
and instead use an ad-hoc ``emoji-description classification'' method that required the creation of a test set with manually labeled emotion-description pairs.


Recent work \citet{guntuku2019studying} combines uses the subdivision technique (Section \ref{}) to study how emoji are used differently in different parts of the world.

The simple and general design of our methods allows them to be easily adapted to intrinsically evaluate emoji embeddings.
We demonstrate this by training our own emoji embeddings using the code from \citet{eisner2016emoji2vec},
but we use $\OddOneOut$ and $\topk$ as the model selection metric.
The semantic categories of emojis are trivial to generate from unicode's full emoji list.%
\footnote{\url{https://unicode.org/Public/emoji/13.0/emoji-test.txt}} 

In addition to intrinsic evaluation of emoji2vec with their custom method, \cite{eisner2016emoji2vec} also deploy their vectors in a downstream sentiment analysis of tweets.
We verify the performance of our tuned emoji embeddings by also evaluating performance on this same sentiment analysis task.
Figure \ref{fig:emoji} shows the results of evaluation, we find that our embeddings were able to reproduce \cite{eisner2016emoji2vec} performance on the downstream task.


%Biblical
\section{Multilingual Content Analysis}
\label{sec:mca}

Third, we use the $\OddOneOut$ and $\topk$ metrics to select the hyperparameters for models trained on 17 ancient languages provided by the CLTK \cite{} library.
This experiment 

In Section \ref{sec:mca} below, we use these 

\subsection{Automatic Test Set Generation with Wikidata}

One of the most difficult and time consuming steps in the process of generating of high quality word embeddings is the creation of a comprehensive test set.
 Consequently, one of the biggest advantages of the $\OddOneOut$ and $\topk$ methods is that compatible test sets can be semi-automatically generated in hundreds of languages.
This is possible using Wikidata. 
This publicly available SPARQL database contains a comprehensive structure of the semantic content contained in Wikipedia
along with its relationship to other items.
Simple queries constructed using the Wikidata Query Service can be used to return semantic categories of words that can be included in a test set.

Using Wikidata we were able to reconstruct all of the semantic categories contained within the Google analogy set with the option of greater customization and more categories.
 Additionally, Wikidata supports the translation of queried items into 461 languages, allowing test sets to easily be converted between languages.
  Though not all items have translated labels in all 461 languages, support is quite extensive and will only get better. 

One of the disadvantages of using Wikidata is that syntactic categories are much harder to construct;
 however, since semantic categories are usually more difficult to generate 
 and require more arbitrary decisions this approach is still very valuable.

\subsection{Classical Languages Experiments}

% Improved Table - Combined Score
\begin{table*}[t]
\centering
%\scalebox{.54}{
\tiny
\begin{tabular}{l|llrr|llrrrrl|rrr}
\toprule
\multicolumn{5}{c}{\bf Corpus} & \multicolumn{7}{c}{\bf Parameters} & \multicolumn{3}{c}{\bf Scores} \\
\midrule
& Language & Test Set & Tokens & Unique Tokens & Model & Type & Dim & Window & LR & Min Count & Lemma & $\OddOneOut$ & $\topk$ & \texttt{Combined} \\
\midrule
    
    \multirow{2}{*}{Hellenic}& greek & ancient greek & \num{37868209} & \num{1877574} & w2v & cbow & 40 & 9 & -1 & 4 & False & 1140 & 8 & 17.86\\
    & greek & modern greek & \num{37868209} & \num{1877574} & fast & sg & 90 & 7 & -1 & 5 & True & 300 & 9 & 19.36\\  
    \midrule
    \multirow{2}{*}{Italic}&latin & latin & \num{17777429} & \num{470790} & w2v & cbow & 50 & 10 & -1 & 7 & False & 2748 & 48 & 96.28\\ 
    &old french  & french & \num{68741} & \num{8343} &  fast & sg & 250 & 6 & -1 & 8  & False & 1 & 7 & 3.20\\
    \midrule
    \multirow{5}{*}{Germanic}&middle english & english (old) & \num{7048144} & \num{314527} & fast & sg & 90 & 5 & -1 & 7 & False & 239 & 7 & 15.48 \\
    &middle high german & german & \num{2090954} & \num{60674} & fast & cbow & 15 & 6 & -1 & 3 & False & 9 & 19 & 13.33 \\
    &old english  & english (old) & \num{104011} & \num{33018} & fast & cbow & 425 & 3 & -1 & 3 & True & 0 & 1  & 1.33 \\
    &old norse & icelandic & \num{458377} & \num{59186} & w2v & cbow & 60 & 10 & -1 & 3 & False & 968 & 2 & 5.98 \\
    &old swedish & swedish & \num{1297740} & \num{116374} &  fast & sg & 50 & 8 & -1 & 5 & False & 50 & 1 & 3.85 \\
    \midrule
    \multirow{8}{*}{Indo-Aryan}&bengali & bengali & \num{5539} & \num{2323} & fast & cbow & 15 & 3 & -1 & 4 & False & 0 & 2 & 1.50 \\
    &gujarati & gujarati & \num{1813} & \num{1140} & fast & sg & 80 & 3 & -1 & 5 & False & 0 & 7 & 1.78 \\
    &hindi & hindi& \num{587655} & \num{55483} & fast & cbow & 45 & 4 & -1 & 8 & False & 263 & 2 & 5.93\\
    &malayalam & malayalam& \num{9235} & \num{5405} & - & - & - & - & - & - & - & - & - & - \\
    &marathi & marathi & \num{797926} & \num{96778} & w2v & sg & 400 & 4 & -1 & 6 & False & 342 & 1 & 3.98 \\
    &punjabi & punjabi & \num{1024075} & \num{31343} & fast & sg & 50 & 8 & -1 & 5 & False & 0 & 1 & 1.3 \\
    &sanskrit & sanskrit & \num{4042204} & \num{896480} & w2v & sg & 35 & 9 & -1 & 10 & False & 1530 & 1 & 3.99\\
    &telugu & telugu & \num{537673} & \num{276330} &  w2v & cbow & 60 & 10 & -1 & 3 & False & 50 & 0 & 1.96 \\
    \midrule
    \multirow{2}{*}{Semitic}&classical arabic & arabic& \num{81306} & \num{20493} & - & - & - & - & - & - & - & - & - & - \\
    &hebrew & hebrew & \num{41378460} & \num{893512} & fast & sg & 30 & 4 & -1 & 3 & False & 1098 & 6 & 13.91 \\  
\bottomrule
\end{tabular}
%}
\caption{The table above provides details for the best model trained for languages supported by CLTK.
Following a tuning process, models were chosen by their $\texttt{Combined Score}$
which is calculated as the harmonic mean of $\topk$ and $\OddOneOut$.
It is important to note that the absolute score for our evaluation metrics are not important in and of themselves,
rather they are important as indicators of a change in embedding quality that the analogy task would fail to show.
To emphasize this, we have reported the raw number of correct answers for each metric.
Using $\OddOneOut$ and $\topk$ allows us to tune models trained on corpora with unique token counts in the thousands instead of the millions.}
\label{table:language}
\end{table*}

An obvious application of $\OddOneOut$ and $\topk$ is in training models for under-resourced languages. 
The Classical Language Toolkit (CLTK) \cite{johnson2014} is one such ongoing project supporting nlp for various classical, low resource, and often historical languages.
To confirm the efficacy of our methods in the low resource setting, we train and tune word embeddings on all languages supported by the Classical Language Toolkit which contain at least one downloadable corpus and a tokenization tool. 
In total this leaves us with $18$ low resource languages.

For each language we preprocess all corpora available through CLTK, using their normalization and tokenization tools where appropriate. 
We then perform a randomized grid search over the key model parameters consisting of 100 samples. 
The key hyperparameters we searched over can be found in Table \ref{table:language}. 
After training the model we perform an intrinsic evaluation for the embeddings on the corresponding language specific test set 
and pick the model that had the highest \texttt{Combined Score} 
which is calculated as the harmonic mean of $\OddOneOut$ and $\topk$.

Although the parameter combinations searched over for each language were the same,
the best model for each language contained a unique set of parameters. 
This supports the need for an intrinsic tuning method for word embeddings so that highest performance can be achieved. 
The tuning results in Table \ref{table:language} also seem to indicate that fastText models perform better than word2vec in low resource scenarios 
 due to the training of n-grams.


Aside from intrinsic evaluation, our methods can be leveraged as a qualitative downstream task we call multilingual content analysis.
The goal of the task is to determine the degree to which some concept or category is captured by a set of word embeddings. 
We implement this by using Wikidata to construct $3$ categories of interest--Biblical Figures, Facets of Hinduism, and Facets of Buddhism--and then evaluate them using $\OddOneOut$ and $\topk$. 
The performance on this task demonstrates the degree to which our models understand the semantics of the chosen topics 
and provides insight regarding the content they were trained on.

With intentionally selected categories we are able draw out the differences in content between the different language models
that in many cases matches our intuition. 
By evaluating on a category of biblical figures we see highest performance from the Hebrew and Latin models followed by other European languages; 
whereas Facets of Buddhism and Facets of Hinduism see high performance from the Indic language models.

Perhaps more interesting, this downstream task also gives rise to circumstances in which $\topk$ performance exceeded $\OddOneOut$. 
In some cases (like Middle High German on Biblical Figures) $\topk$ merely surpasses $\OddOneOut$ 
and in others (such as Gujarati on Facets of Hinduism) it is the only metric to achieve a performance score. 
It is likely that a complex interaction between the model and the category being evaluated on can result in either method achieving better performance, 
thus it is valuable to have both methods at our disposal. 
Since this exact relationship is still not fully understood we tune and evaluate models using both metrics 
and compute the harmonic mean to choose the most accurate model.

\begin{figure*}
\centering
\resizebox{6in}{2in}{\includegraphics{biblical-raw.pdf}}
\resizebox{6in}{2in}{\includegraphics{buddhism-raw.pdf}}
\resizebox{6in}{2in}{\includegraphics{hinduism-raw.pdf}}
%\includegraphics{buddhism-raw.pdf} 
%\includegraphics{hinduism-raw.pdf} 
%}

\caption{
    (\textbf{Top})
    High performance on the Biblical Figures category indicates some level of biblical influence via the corpus. 
    Interestingly, we see that greek embeddings optimized on the Modern Greek test set significantly outperformed the embeddings optimized for the Ancient Greek. 
    This matches our intuition that things of a biblical nature have had a greater influence on Modern Greek than Ancient Greek.
    (\textbf{Middle})
    Though many of the Indic language embeddings performed well on the Facets of Buddhism, 
    surprisingly so did our Latin and Hebrew embeddings. 
    This leads us to believe that some Buddhist concepts and words are shared by corpora spanning languages as diverse as Hebrew, Latin, and Hindi. 
    At the same time, it should also be noted that Latin and Hebrew were two of the largest models trained compared to other classical languages and thus also likely benefit from greater resource richness.
    (\textbf{Bottom})
    Similar to Figure \ref{fig:biblical} we see languages more closely related to the topic of the category achieving better performance, in this case primarily the Indic languages. 
    Interstingly, our Bengali embeddings performed poorly on this category suggesting that the corpus did not refer heavily to the Hindu context.
    }
\end{figure*}

%animals
%\begin{figure*}
%\centering
%\includegraphics{animals-raw.pdf} 
%\caption{}
%\label{fig:multilingual}
%\end{figure*}

\section{Related Work}
\label{sec:related}



%\section{Related Work}
%\label{sec:related}



%\subsection{Nate}
%
%Look at the way these papers run experiments:
%
%\cite{tifrea2018poincar,meng2019spherical}
%
%Datasets at: \url{https://aclweb.org/aclwiki/Similarity_(State_of_the_art)}
%
%Use wikidata to automatically generate classes: https://www.wikidata.org/wiki/Q43689 
%https://pywikidata.readthedocs.io/en/latest/
%
%wikidata church fathers in latin: \url{https://query.wikidata.org/#SELECT%20%3Fperson%20%3FpersonLabel%0AWHERE%20%7B%0A%20%20%3Fperson%20wdt%3AP361%20wd%3AQ182603.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22la%22.%20%7D%0A%7D}
%
%SPARQL Tutorial: \url{https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial}
%
%Can we use wikidata to automatically generate good evaluations?
%
%\section{Discussion}
%\label{sec:discussion}



%%% End of my stuff 


%\section{Electronically-available resources}
%
%ACL provides this description and accompanying style files at
%\begin{quote}
%\url{https://2020.emnlp.org/files/emnlp2020-templates.zip}
%\end{quote}
%We strongly recommend the use of these style files, which have been appropriately tailored for the EMNLP 2020 proceedings.
%
%\paragraph{\LaTeX-specific details:}
%The templates include the \LaTeX2e{} source (\texttt{\small emnlp2020.tex}),
%the \LaTeX2e{} style file used to format it (\texttt{\small emnlp2020.sty}),
%an ACL bibliography style (\texttt{\small acl\_natbib.bst}),
%an example bibliography (\texttt{\small emnlp2020.bib}),
%and the bibliography for the ACL Anthology (\texttt{\small anthology.bib}).
%
%
%\section{Length of Submission}
%\label{sec:length}
%
%The conference accepts submissions of long papers and short papers.
%Long papers may consist of up to eight (8) pages of content plus unlimited pages for references.
%Upon acceptance, final versions of long papers will be given one additional page -- up to nine (9) pages of content plus unlimited pages for references -- so that reviewers' comments can be taken into account.
%Short papers may consist of up to four (4) pages of content, plus unlimited pages for references.
%Upon acceptance, short papers will be given five (5) pages in the proceedings and unlimited pages for references. 
%For both long and short papers, all illustrations and tables that are part of the main text must be accommodated within these page limits, observing the formatting instructions given in the present document.
%Papers that do not conform to the specified length and formatting requirements are subject to be rejected without review.
%
%The conference encourages the submission of additional material that is relevant to the reviewers but not an integral part of the paper.
%There are two such types of material: appendices, which can be read, and non-readable supplementary materials, often data or code.
%Additional material must be submitted as separate files, and must adhere to the same anonymity guidelines as the main paper.
%The paper must be self-contained: it is optional for reviewers to look at the supplementary material.
%Papers should not refer, for further detail, to documents, code or data resources that are not available to the reviewers.
%Refer to Appendices~\ref{sec:appendix} and \ref{sec:supplemental} for further information. 
%
%Workshop chairs may have different rules for allowed length and whether supplemental material is welcome.
%As always, the respective call for papers is the authoritative source.
%
%
%\section{Anonymity}
%As reviewing will be double-blind, papers submitted for review should not include any author information (such as names or affiliations). Furthermore, self-references that reveal the author's identity, \emph{e.g.},
%\begin{quote}
%We previously showed \citep{Gusfield:97} \ldots
%\end{quote}
%should be avoided. Instead, use citations such as 
%\begin{quote}
%\citet{Gusfield:97} previously showed\ldots
%\end{quote}
%Please do not use anonymous citations and do not include acknowledgements.
%\textbf{Papers that do not conform to these requirements may be rejected without review.}
%
%Any preliminary non-archival versions of submitted papers should be listed in the submission form but not in the review version of the paper.
%Reviewers are generally aware that authors may present preliminary versions of their work in other venues, but will not be provided the list of previous presentations from the submission form.
%
%Once a paper has been accepted to the conference, the camera-ready version of the paper should include the author's names and affiliations, and is allowed to use self-references.
%
%\paragraph{\LaTeX-specific details:}
%For an anonymized submission, ensure that {\small\verb|\aclfinalcopy|} at the top of this document is commented out, and that you have filled in the paper ID number (assigned during the submission process on softconf) where {\small\verb|***|} appears in the {\small\verb|\def\aclpaperid{***}|} definition at the top of this document.
%For a camera-ready submission, ensure that {\small\verb|\aclfinalcopy|} at the top of this document is not commented out.
%
%
%\section{Multiple Submission Policy}
%
%EMNLP 2020 will not consider any paper that is under review in a journal or another conference at the time of submission, and submitted papers must not be submitted elsewhere during the EMNLP 2020 review period. This policy covers all refereed and archival conferences and workshops (e.g., COLING, NeurIPS, ACL workshops). For example, a paper under review at an ACL workshop cannot be dual-submitted to EMNLP 2020. The only exception is that a paper can be dual-submitted to both EMNLP 2020 and an EMNLP workshop which has its submission deadline falling after our original notification date of August 8, 2020. In addition, we will not consider any paper that overlaps significantly in content or results with papers that will be (or have been) published elsewhere. 
%
%Authors submitting more than one paper to EMNLP 2020 must ensure that their submissions do not overlap significantly ($>25$\%) with each other in content or results.
%
%\section{Formatting Instructions}
%
%Manuscripts must be in two-column format.
%Exceptions to the two-column format include the title, authors' names and complete addresses, which must be centered at the top of the first page, and any full-width figures or tables (see the guidelines in Section~\ref{ssec:title-authors}).
%\textbf{Type single-spaced.}
%Start all pages directly under the top margin.
%The manuscript should be printed single-sided and its length should not exceed the maximum page limit described in Section~\ref{sec:length}.
%Pages should be numbered in the version submitted for review, but \textbf{pages should not be numbered in the camera-ready version}.
%
%\paragraph{\LaTeX-specific details:}
%The style files will generate page numbers when {\small\verb|\aclfinalcopy|} is commented out, and remove them otherwise.
%
%
%\subsection{File Format}
%\label{sect:pdf}
%
%For the production of the electronic manuscript you must use Adobe's Portable Document Format (PDF).
%Please make sure that your PDF file includes all the necessary fonts (especially tree diagrams, symbols, and fonts with Asian characters).
%When you print or create the PDF file, there is usually an option in your printer setup to include none, all or just non-standard fonts.
%Please make sure that you select the option of including ALL the fonts.
%\textbf{Before sending it, test your PDF by printing it from a computer different from the one where it was created.}
%Moreover, some word processors may generate very large PDF files, where each page is rendered as an image.
%Such images may reproduce poorly.
%In this case, try alternative ways to obtain the PDF.
%One way on some systems is to install a driver for a postscript printer, send your document to the printer specifying ``Output to a file'', then convert the file to PDF.
%
%It is of utmost importance to specify the \textbf{A4 format} (21 cm x 29.7 cm) when formatting the paper.
%Print-outs of the PDF file on A4 paper should be identical to the hardcopy version.
%If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chairs as soon as possible.
%
%\paragraph{\LaTeX-specific details:}
%PDF files are usually produced from \LaTeX{} using the \texttt{\small pdflatex} command.
%If your version of \LaTeX{} produces Postscript files, \texttt{\small ps2pdf} or \texttt{\small dvipdf} can convert these to PDF.
%To ensure A4 format in \LaTeX, use the command {\small\verb|\special{papersize=210mm,297mm}|}
%in the \LaTeX{} preamble (below the {\small\verb|\usepackage|} commands) and use \texttt{\small dvipdf} and/or \texttt{\small pdflatex}; or specify \texttt{\small -t a4} when working with \texttt{\small dvips}.
%
%\subsection{Layout}
%\label{ssec:layout}
%
%Format manuscripts two columns to a page, in the manner these
%instructions are formatted.
%The exact dimensions for a page on A4 paper are:
%
%\begin{itemize}
%\item Left and right margins: 2.5 cm
%\item Top margin: 2.5 cm
%\item Bottom margin: 2.5 cm
%\item Column width: 7.7 cm
%\item Column height: 24.7 cm
%\item Gap between columns: 0.6 cm
%\end{itemize}
%
%\noindent Papers should not be submitted on any other paper size.
%If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chairs above as soon as possible.
%
%\subsection{Fonts}
%
%For reasons of uniformity, Adobe's \textbf{Times Roman} font should be used.
%If Times Roman is unavailable, you may use Times New Roman or \textbf{Computer Modern Roman}.
%
%Table~\ref{font-table} specifies what font sizes and styles must be used for each type of text in the manuscript.
%
%\begin{table}
%\centering
%\begin{tabular}{lrl}
%\hline \textbf{Type of Text} & \textbf{Font Size} & \textbf{Style} \\ \hline
%paper title & 15 pt & bold \\
%author names & 12 pt & bold \\
%author affiliation & 12 pt & \\
%the word ``Abstract'' & 12 pt & bold \\
%section titles & 12 pt & bold \\
%subsection titles & 11 pt & bold \\
%document text & 11 pt  &\\
%captions & 10 pt & \\
%abstract text & 10 pt & \\
%bibliography & 10 pt & \\
%footnotes & 9 pt & \\
%\hline
%\end{tabular}
%\caption{\label{font-table} Font guide. }
%\end{table}
%
%\paragraph{\LaTeX-specific details:}
%To use Times Roman in \LaTeX2e{}, put the following in the preamble:
%\begin{quote}
%\small
%\begin{verbatim}
%\usepackage{times}
%\usepackage{latexsym}
%\end{verbatim}
%\end{quote}
%
%
%\subsection{Ruler}
%A printed ruler (line numbers in the left and right margins of the article) should be presented in the version submitted for review, so that reviewers may comment on particular lines in the paper without circumlocution.
%The presence or absence of the ruler should not change the appearance of any other content on the page.
%The camera ready copy should not contain a ruler.
%
%\paragraph{Reviewers:}
%note that the ruler measurements may not align well with lines in the paper -- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
%In most cases one would expect that the approximate location will be adequate, although you can also use fractional references (\emph{e.g.}, this line ends at mark $295.5$).
%
%\paragraph{\LaTeX-specific details:}
%The style files will generate the ruler when {\small\verb|\aclfinalcopy|} is commented out, and remove it otherwise.
%
%\subsection{Title and Authors}
%\label{ssec:title-authors}
%
%Center the title, author's name(s) and affiliation(s) across both columns.
%Do not use footnotes for affiliations.
%Place the title centered at the top of the first page, in a 15-point bold font.
%Long titles should be typed on two lines without a blank line intervening.
%Put the title 2.5 cm from the top of the page, followed by a blank line, then the author's names(s), and the affiliation on the following line.
%Do not use only initials for given names (middle initials are allowed).
%Do not format surnames in all capitals (\emph{e.g.}, use ``Mitchell'' not ``MITCHELL'').
%Do not format title and section headings in all capitals except for proper names (such as ``BLEU'') that are
%conventionally in all capitals.
%The affiliation should contain the author's complete address, and if possible, an electronic mail address.
%
%The title, author names and addresses should be completely identical to those entered to the electronical paper submission website in order to maintain the consistency of author information among all publications of the conference.
%If they are different, the publication chairs may resolve the difference without consulting with you; so it is in your own interest to double-check that the information is consistent.
%
%Start the body of the first page 7.5 cm from the top of the page.
%\textbf{Even in the anonymous version of the paper, you should maintain space for names and addresses so that they will fit in the final (accepted) version.}
%
%
%\subsection{Abstract}
%Use two-column format when you begin the abstract.
%Type the abstract at the beginning of the first column.
%The width of the abstract text should be smaller than the
%width of the columns for the text in the body of the paper by 0.6 cm on each side.
%Center the word \textbf{Abstract} in a 12 point bold font above the body of the abstract.
%The abstract should be a concise summary of the general thesis and conclusions of the paper.
%It should be no longer than 200 words.
%The abstract text should be in 10 point font.
%
%\subsection{Text}
%Begin typing the main body of the text immediately after the abstract, observing the two-column format as shown in the present document.
%
%Indent 0.4 cm when starting a new paragraph.
%
%\subsection{Sections}
%
%Format section and subsection headings in the style shown on the present document.
%Use numbered sections (Arabic numerals) to facilitate cross references.
%Number subsections with the section number and the subsection number separated by a dot, in Arabic numerals.
%
%\subsection{Footnotes}
%Put footnotes at the bottom of the page and use 9 point font.
%They may be numbered or referred to by asterisks or other symbols.\footnote{This is how a footnote should appear.}
%Footnotes should be separated from the text by a line.\footnote{Note the line separating the footnotes from the text.}
%
%\subsection{Graphics}
%
%Place figures, tables, and photographs in the paper near where they are first discussed, rather than at the end, if possible.
%Wide illustrations may run across both columns.
%Color is allowed, but adhere to Section~\ref{ssec:accessibility}'s guidelines on accessibility.
%
%\paragraph{Captions:}
%Provide a caption for every illustration; number each one sequentially in the form:
%``Figure 1. Caption of the Figure.''
%``Table 1. Caption of the Table.''
%Type the captions of the figures and tables below the body, using 10 point text.
%Captions should be placed below illustrations.
%Captions that are one line are centered (see Table~\ref{font-table}).
%Captions longer than one line are left-aligned (see Table~\ref{tab:accents}).
%
%\begin{table}
%\centering
%\begin{tabular}{lc}
%\hline
%\textbf{Command} & \textbf{Output}\\
%\hline
%\verb|{\"a}| & {\"a} \\
%\verb|{\^e}| & {\^e} \\
%\verb|{\`i}| & {\`i} \\ 
%\verb|{\.I}| & {\.I} \\ 
%\verb|{\o}| & {\o} \\
%\verb|{\'u}| & {\'u}  \\ 
%\verb|{\aa}| & {\aa}  \\\hline
%\end{tabular}
%\begin{tabular}{lc}
%\hline
%\textbf{Command} & \textbf{Output}\\
%\hline
%\verb|{\c c}| & {\c c} \\ 
%\verb|{\u g}| & {\u g} \\ 
%\verb|{\l}| & {\l} \\ 
%\verb|{\~n}| & {\~n} \\ 
%\verb|{\H o}| & {\H o} \\ 
%\verb|{\v r}| & {\v r} \\ 
%\verb|{\ss}| & {\ss} \\
%\hline
%\end{tabular}
%\caption{Example commands for accented characters, to be used in, \emph{e.g.}, \BibTeX\ names.}\label{tab:accents}
%\end{table}
%
%\paragraph{\LaTeX-specific details:}
%The style files are compatible with the caption and subcaption packages; do not add optional arguments.
%\textbf{Do not override the default caption sizes.}
%
%
%\subsection{Hyperlinks}
%Within-document and external hyperlinks are indicated with Dark Blue text, Color Hex \#000099.
%
%\subsection{Citations}
%Citations within the text appear in parentheses as~\citep{Gusfield:97} or, if the author's name appears in the text itself, as \citet{Gusfield:97}.
%Append lowercase letters to the year in cases of ambiguities.  
%Treat double authors as in~\citep{Aho:72}, but write as in~\citep{Chandra:81} when more than two authors are involved. Collapse multiple citations as in~\citep{Gusfield:97,Aho:72}. 
%
%Refrain from using full citations as sentence constituents.
%Instead of
%\begin{quote}
%  ``\citep{Gusfield:97} showed that ...''
%\end{quote}
%write
%\begin{quote}
%``\citet{Gusfield:97} showed that ...''
%\end{quote}
%
%\begin{table*}
%\centering
%\begin{tabular}{lll}
%\hline
%\textbf{Output} & \textbf{natbib command} & \textbf{Old ACL-style command}\\
%\hline
%\citep{Gusfield:97} & \small\verb|\citep| & \small\verb|\cite| \\
%\citealp{Gusfield:97} & \small\verb|\citealp| & no equivalent \\
%\citet{Gusfield:97} & \small\verb|\citet| & \small\verb|\newcite| \\
%\citeyearpar{Gusfield:97} & \small\verb|\citeyearpar| & \small\verb|\shortcite| \\
%\hline
%\end{tabular}
%\caption{\label{citation-guide}
%Citation commands supported by the style file.
%The style is based on the natbib package and supports all natbib citation commands.
%It also supports commands defined in previous ACL style files for compatibility.
%}
%\end{table*}
%
%\paragraph{\LaTeX-specific details:}
%Table~\ref{citation-guide} shows the syntax supported by the style files.
%We encourage you to use the natbib styles.
%You can use the command {\small\verb|\citet|} (cite in text) to get ``author (year)'' citations as in \citet{Gusfield:97}.
%You can use the command {\small\verb|\citep|} (cite in parentheses) to get ``(author, year)'' citations as in \citep{Gusfield:97}.
%You can use the command {\small\verb|\citealp|} (alternative cite without  parentheses) to get ``author year'' citations (which is useful for  using citations within parentheses, as in \citealp{Gusfield:97}).
%
%
%\subsection{References}
%Gather the full set of references together under the heading \textbf{References}; place the section before any Appendices. 
%Arrange the references alphabetically by first author, rather than by order of occurrence in the text.
%
%Provide as complete a citation as possible, using a consistent format, such as the one for \emph{Computational Linguistics\/} or the one in the  \emph{Publication Manual of the American 
%Psychological Association\/}~\citep{APA:83}.
%Use full names for authors, not just initials.
%
%Submissions should accurately reference prior and related work, including code and data.
%If a piece of prior work appeared in multiple venues, the version that appeared in a refereed, archival venue should be referenced.
%If multiple versions of a piece of prior work exist, the one used by the authors should be referenced.
%Authors should not rely on automated citation indices to provide accurate references for prior and related work.
%
%The following text cites various types of articles so that the references section of the present document will include them.
%\begin{itemize}
%\item Example article in journal: \citep{Ando2005}.
%\item Example article in proceedings, with location: \citep{borschinger-johnson-2011-particle}.
%\item Example article in proceedings, without location: \citep{andrew2007scalable}.
%\item Example arxiv paper: \citep{rasooli-tetrault-2015}. 
%\end{itemize}
%
%
%\paragraph{\LaTeX-specific details:}
%The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
%If your own bib file is named \texttt{\small emnlp2020.bib}, then placing the following before any appendices in your \LaTeX{}  file will generate the references section for you:
%\begin{quote}\small
%\verb|\bibliographystyle{acl_natbib}|\\
%\verb|\bibliography{emnlp2020}|
%\end{quote}
%
%You can obtain the complete ACL Anthology as a Bib\TeX\ file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
%To include both the anthology and your own bib file, use the following instead of the above.
%\begin{quote}\small
%\verb|\bibliographystyle{acl_natbib}|\\
%\verb|\bibliography{anthology,emnlp2020}|
%\end{quote}
%
%
%\subsection{Digital Object Identifiers}
%As part of our work to make ACL materials more widely used and cited outside of our discipline, ACL has registered as a CrossRef member, as a registrant of Digital Object Identifiers (DOIs), the standard for registering permanent URNs for referencing scholarly materials.
%
%All camera-ready references are required to contain the appropriate DOIs (or as a second resort, the hyperlinked ACL Anthology Identifier) to all cited works.
%Appropriate records should be found for most materials in the current ACL Anthology at \url{http://aclanthology.info/}.
%As examples, we cite \citep{goodman-etal-2016-noise} to show you how papers with a DOI will appear in the bibliography.
%We cite \citep{harper-2014-learning} to show how papers without a DOI but with an ACL Anthology Identifier will appear in the bibliography.
%
%\paragraph{\LaTeX-specific details:}
%Please ensure that you use Bib\TeX\ records that contain DOI or URLs for any of the ACL materials that you reference.
%If the Bib\TeX{} file contains DOI fields, the paper title in the references section will appear as a hyperlink to the DOI, using the hyperref \LaTeX{} package.
%
%
%\subsection{Appendices}
%Appendices, if any, directly follow the text and the
%references (but only in the camera-ready; see Appendix~\ref{sec:appendix}).
%Letter them in sequence and provide an informative title:
%\textbf{Appendix A. Title of Appendix}.
%
%\section{Accessibility}
%\label{ssec:accessibility}
%
%In an effort to accommodate people who are color-blind (as well as those printing to paper), grayscale readability is strongly encouraged.
%Color is not forbidden, but authors should ensure that tables and figures do not rely solely on color to convey critical distinctions.
%A simple criterion:
%All curves and points in your figures should be clearly distinguishable without color.
%
%\section{Translation of non-English Terms}
%
%It is also advised to supplement non-English characters and terms with appropriate transliterations and/or translations since not all readers understand all such characters and terms.
%Inline transliteration or translation can be represented in the order of:
%\begin{center}
%\begin{tabular}{c}
%original-form \\
%transliteration \\
%``translation''
%\end{tabular}
%\end{center}
%
%\section{\LaTeX{} Compilation Issues}
%You may encounter the following error during compilation: 
%\begin{quote}
%{\small\verb|\pdfendlink|} ended up in different nesting level than {\small\verb|\pdfstartlink|}.
%\end{quote}
%This happens when \texttt{\small pdflatex} is used and a citation splits across a page boundary.
%To fix this, the style file contains a patch consisting of two lines:
%(1) {\small\verb|\RequirePackage{etoolbox}|} (line 455 in \texttt{\small emnlp2020.sty}), and
%(2) A long line below (line 456 in \texttt{\small emnlp2020.sty}).
%
%If you still encounter compilation issues even with the patch enabled, disable the patch by commenting the two lines, and then disable the \texttt{\small hyperref} package by loading the style file with the \texttt{\small nohyperref} option:
%
%\noindent
%{\small\verb|\usepackage[nohyperref]{emnlp2020}|}
%
%\noindent
%Then recompile, find the problematic citation, and rewrite the sentence containing the citation. (See, {\em e.g.}, \url{http://tug.org/errors.html})
%
%\section*{Acknowledgments}
%
%The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
%Do not include this section when submitting your paper for review.

\clearpage
%\nocite{*}
\bibliographystyle{acl_natbib}
%\interlinepenalty=10000
\bibliography{emnlp2020}


\end{document}
