%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
%\usepackage[hyperref]{emnlp2020}
\usepackage{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsmath,amsfonts}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{siunitx}
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\usepackage{etoolbox}
\makeatletter
\patchcmd\@combinedblfloats{\box\@outputbox}{\unvbox\@outputbox}{}{\errmessage{\noexpand patch failed}}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% latex functions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\ltwo}[1]{\lVert{#1}\rVert}
\newcommand{\indicator}[1]{\mathbbm{1}\!\left[{#1}\right]}

\newcommand{\R}{\mathbb R}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\FindMostSimilar}{\texttt{Sim}}
\DeclareMathOperator{\OddOneOut}{\texttt{OddOneOut}}
\DeclareMathOperator{\topk}{\texttt{Topk}}
\DeclareMathOperator{\analogy}{\texttt{Analogy}}

\newcommand{\fixme}[1]{{\color{red}\itshape \textbf{FIXME:} {#1}}}

% what were these here for?
%\newcommand\BibTeX{B\textsc{ib}\TeX}
%\special{papersize=210mm,297mm}

% Paper
\title{Instructions for EMNLP 2020 Proceedings}

\author{Nathan Stringham\\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Mike Izbicki \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    The analogy task introduced by \citet{mikolov2013efficient} has become the standard method for tuning the hyperparameters for word embedding models.
    It has an intrinsic appeal and has been shown to correlate well with downstream tasks.
    In this paper, however, we show that the analogy task is unsuitable for low-resource settings for two reasons: (1) it requires large amounts of data to make measurable progress on,
    and (2) it is not even well-defined in some settings.
    We solve this problem by introducing the $\OddOneOut$ and $\topk$ tasks,
    which are specifically designed for model selection in the low-resource setting.
    Performance on these tasks also correlates well with downstream tasks,
    but these tasks are more sensitive to word embedding performance when trained on smaller datasets.
    We use these metrics to successfully tune hyperparameters for 18 low-resource languages provided by the Classical Languages ToolKit.    
    The smallest of these languages (Ancient Gujarati) has only 1813 unique tokens available.

    %The standard method for improving results on low-resource languages is to develop new machine learning algorithms that more efficiently use the available data.
    %In this paper, however, we argue that 
%
%There has been a strong push in recent years for improved natural language processing on low-resource languages.
%Little data exists for these languages,
%and so trained machine learning models have low accuracies.
%Recent work has focused on solving this problem by developing more sample-efficient algorithms that can achieve higher accuracies with less data.
%We argue, however, that standard evaluation tasks are not sensitive enough to measure model quality in the low-resource setting,
%and new performance measures tailored to the low-resource setting are required.
%%In this paper, we focus on evaluating word embeddings.
%Specifically, we argue that the standard analogy task is not suitable for the low-resource setting and we introduce two new evaluation metrics designed for the low-resource setting.
%%The analogy task is not suitable for low-resource languages,
%however, because good performance requires a large amount of training data and meaningful analogies can be difficult to create.
%As a result, researchers in working in low-resource settings have created many ad-hoc methods for 
%
 %%however, evidence suggests that analogies may not perform equally well across training contexts.
 %%Specifically, this method has proven non-trivial to adapt to languages beyond English and empirical evidence suggests that it requires large amounts of training data.
 %As a result, those working in low resource or specialized settings have lacked a standard method to evaluate the quality of word embeddings.
 %To address this problem, we propose two different intrinsic evaluation methods, which achieve greater performance sensitivity than the analogy task in low resource settings.
  %We demonstrate the flexibility of these methods by using them to tune embeddings for $18$ low resource languages as well as the full list of unicode emojis.
   %Furthermore, our methods are designed such that custom test sets can be developed semi-automatically in over $400$ different languages, which helps increase access for research in areas previously inhibited by low amounts of data.

\end{abstract}


%%% Copied this from other document
\section{Introduction}
There has been a strong push in recent years for improved natural language processing on low-resource languages.
Little data exists for these languages,
and so trained machine learning models have low accuracies.
Recent work has focused on solving this problem by developing more sample-efficient algorithms that can achieve higher accuracies with less data.
We argue, however, that standard evaluation tasks are not sensitive enough to measure model quality in the low-resource setting,
and new performance measures tailored to the low-resource setting are required.
%In this paper, we focus on evaluating word embeddings.
Specifically, we argue that the standard analogy task is not suitable for the low-resource setting and we introduce two new evaluation metrics designed for the low-resource setting.
\label{sec:intro}

\citet{mikolov2013efficient} introduced the word2vec model for training word embeddings and the analogy task for evaluating them.
In the analogy task, the goal is to answer questions similar to
\begin{equation*}
    \texttt{Man is to King as Woman is to ?}
\end{equation*}
%\begin{equation}
%\texttt{king} - \texttt{man} + \texttt{woman} \approx \texttt{queen}
%.
%\end{equation}
by exploiting the linear structure of the word embeddings.
Follow on work has focused on developing better algorithms for training word embeddings as measured by performance on the analogy task.
GloVe \citep{pennington2014glove} and FastText \citep{bojanowski2016enriching} are the two most prominent examples,
although dozens of alternatives have now been proposed.
The analogy task has proven so successful for English language research because it correlates well with performance on a wide range of downstream tasks \citep{wang2019evaluating}.

While most word embeddings algorithms focus on the high-resource domain,
several recent works have developed new algorithms designed for training models in the low-resource setting \cite{adams-etal-2017-cross,jiang2018learningword,gupta-etal-2019-improving,jungmaier-etal-2020-dirichlet}.
Unfortunately, each of these works evaluate their method only in a simulated low-resource environment using English text and not on any actual low-resource languages.
They do this specifically because no evaluation metrics exist that are suitable for their low-resource target languages.
They also do not use the analogy task to measure performance.
Instead, they use word similarity tasks.
These tasks include datasets based on the English-language WordNet:
WordSim Similarity \citep{zesch2008using}%
%\footnote{
%The WordSim Similarity \citep{zesch2008using} task is the most similar to our proposed $\topk$ and $\OddOneOut$ tasks,
%since they use the wiktionary database\footnote{\url{https://wiktionary.org}} and we use the wikidata database.
%The difference is that: their test set construction technique requires a complicated graph algorithm that requires input from a word bank (they use WordNet for English and GermaNet for German).
%In contrast, our techniques are algorithmically simpler, are easy to interpret, require no external data inputs, and are easily extended to the 400 languages available on wikidata.
%}
WordSim Relatedness \citep{agirre2009study},
and datasets based on human annotation of English language word pairs:
WordSim363 \citep{finkelstein2001placing},
Mechanical Turk \citep{radinsky2011word},
MEN \citep{bruni2012distributional}.
These word similarity tasks are more sensitive to their low-resource experimental design than the analogy task,
but they cannot be easily used in non-English languages because they require the manual generation of new datasets for each language.

%Due to Zipf's law, some words will be very infrequent.
%Analogies are known to not work well on these infrequently used words.

Perhaps surprisingly, there is growing body of digital humanities work where English language text is low-resource.
This occurs when researchers attempt to compare how to bodies of text use words differently over different time periods \cite[e.g.][]{hamilton2016diachronic,hamilton2016cultural,kutuzov2018diachronic,kozlowski2019geometry,dubossarsky2017outta,tang2018state,szymanski2017temporal,liang2018dynamic,chen2017understanding}
or different political ideologies \citet{azarbonyad2017words}
or different twitter users \cite{kulkarni2015statistically}.

Our contributions can be summarized with the following three points.

\label{sec:contributions}
\begin{enumerate}
    %\item 
    %We show that the analogy task introduced by \citet{mikolov2013efficient} is not suitable for evaluating low-resource word embeddings.
    \item 
    We introduce the first word embedding evaluation tasks designed specifically for the low-resource setting, $\OddOneOut$ and $\topk$.
    Code for computing these metrics is released as an open source Python library.%
    \footnote{
        URL hidden for blind review.
    }
    \item 
    We introduce a method for automatically generating test datasets for the $\OddOneOut$ and $\topk$ tasks in the 581 languages supported by the wikidata project.%
        \footnote{For the full list of languages supported, see \url{https://www.wikidata.org/wiki/Help:Wikimedia_language_codes/lists/all} \fixme{You wrote 461 without a citation.  Which is correct?}}
    All previous evaluation methods work on only a small number of languages (typically just English),
    and require significant manual work to adapt to new languages.
	\item 
    We perform the largest existing multilingual evaluation on low-resource languages.
    Specifically, we provide word embeddings for the 18 languages of the Classical Languages ToolKit (CLTK) library \fixme{bibtex format is wrong \citep{johnson2014}}.
    We further introduce a new downstream task called the Language Comparison Task (LCT) that lets us map which topics are included in these classical language corpora.
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=1\columnwidth]{size_vs_acc.pdf} 
\caption{The plot above compares the regions of effectiveness for our evaluation metrics. 
The analogy task fails to measure change in accuracy of the embeddings until the number of unique words in the training dataset reaches  $2^{16}$, much later than both $\OddOneOut$ and $\topk$. 
Though $\OddOneOut$ seems the clear victor of these methods, experimentation in \ref{sec:experiments} shows that $\topk$ works better in some circumstances.}
\label{fig:size_vs_acc}
\end{figure}

The remainder of the paper is organized as follows.
Section \ref{sec:method} formally defines the $\topk$ and $\OddOneOut$ tasks.
Section \ref{sec:experiments} empirically demonstrates that these tasks are better than the analogy task in low-resource settings.
We use a synthetic English language experimental design common in previous work,
and demonstrate the versatility of these evaluation metrics by applying them to an emoji embedding task for which the analogy task is not even well defined.
Section \ref{sec:mca} computes word embeddings for the 18 languages provided by CLTK.
We further introduce our technique for using wikidata to automatically generate the test sets for the $\OddOneOut$ and $\topk$ tasks and the LCT task and provide a semantic analysis of the topics covered in each of these 18 language copora.
Section \ref{sec:discussion} concludes by discussing how extensions to this work could serve communities working with low-resource languages.

\section{Evaluation Methods}
\label{sec:method}
The $\OddOneOut$ and $\topk$ tasks are simple and generally applicable.
Both tasks require a test set in the same easy to generate format.
The test set consists of a list of categories,
and each category contains a list of words that belong to that category.
For example, the category \texttt{fruit} might contain the words \texttt{banana}, \texttt{apple}, and \texttt{orange}. \fixme{use a real example.}
The $\OddOneOut$ task measures a model's ability to identify words that are unrelated to the category,
and the $\topk$ task measures a model's ability to identify words that are related to the category.

Formally,
assume that there are $m$ categories,
that each category has $n$ words%
\footnote{
    In practice, the number of words per category can vary for each category,
    however for notational simplicity we assume that each category has the same number of words.
},
that there are $v$ total words in the vocabulary,
and that the words are embedded into $\R^d$.
Let $c_{i,j}$ be the $j$th word in category $i$,
and let $C_i = \{c_{i,1}, c_{i,2}, ..., c_{i,n}\}$ be the $i$th category.

\subsection{The $\topk$ method}
Let $\FindMostSimilar(k,w)$ return the $k$ most similar words in the vocabulary to $w$.
We use the cosine distance in all our experiments,
but any distance metric can be used.
Next, define the $\topk$ score for class $i$ to be
\begin{equation}
    \topk(k,i) = \frac{1}{n}\sum_{j=1}^n \frac{1}{k}\sum_{\mathrlap{x \in \FindMostSimilar(k, c_{i,j})}} \indicator{x \in C_i}
\end{equation}
and the $\topk$ score for the entire evaluation set to be
\begin{equation}
    \topk(k) = \frac{1}{m}\sum_{i=1}^m \topk(k,i)
    .
\end{equation}

The runtime of $\FindMostSimilar$ is $O(dvk)$.\footnote{
    We use gensim's implementation of $\FindMostSimilar$,
    which uses the naive loop strategy for computing the nearest neighbor.
    Data structures like the $k$d-tree or cover tree could potentially be used to speed up this search,
    but we did not find such data structures necessary.
}
So the runtime of $\topk(k,i)$ is $O(dnk^2v)$
and the runtime of $\topk(k)$ is $O(dnmk^2v)$.
Typically $k$ is small (we recommend $k=3$ in our experiments),
and so the runtime is linear in all of the interesting parameters.
In particular, it is linear in both the size of our vocabulary, the number of categories in the test set, and the size of the categories.

\subsection{The $\OddOneOut$ method}
Define the $\OddOneOut$ score of a set $S$ with $k$ words with respect to a word $w\not\in S$ as

%\begin{equation}
%    \OddOneOut(S,w) = \indicator{w = \hat w},
%\end{equation}
%where
%\begin{equation}
%    \hat w = \argmax_{x \in S\cup\{w\}} \ltwo{x - \mu}
%    \quad
%    \text{and}
%    \quad
%    \mu = \frac1{k+1}\bigg(w + \sum_{i=1}^k{s_i}\bigg)
%    .
%\end{equation}

\begin{equation}
    \OddOneOut(S,w) = \indicator{w = \hat w},
\end{equation}
where
\begin{equation}
    \hat w = \argmax_{x \in S\cup\{w\}} \ltwo{x - \mu}
\end{equation}
\begin{equation}
    %\quad
    \text{and}
    \quad
    \mu = \frac1{k+1}\bigg(w + \sum_{i=1}^k{s_i}\bigg)
    .
\end{equation}


We define the $k$th order $\OddOneOut$ score of a category $i$ to be
\begin{equation}
    \OddOneOut(k,i) = \frac 1 {|P|} \sum_{\mathclap{(S,w) \in P}} \OddOneOut(S,w)
    \label{eq:OOOki}
\end{equation}
where
\begin{multline}
    P = \{ (S, w) : S \text{~is a combination of $k$ words}\\\text{from $C_i$}, \text{and~} w \in V-C_i \}
    .
    \label{eq:P}
\end{multline}
In Equation \eqref{eq:P} above,
the total number of values that $S$ can take is ${n \choose k} = O(n^k)$,
and the total number of values that $w$ can take is $O(v)$,
so $|P| = O(n^kv)$.
Finally, we define the $k$-th order $\OddOneOut$ score of the entire evaluation set to be
\begin{equation}
    \OddOneOut(k) = \frac 1 m \sum_{i=1}^m \OddOneOut(k,i)
    .
\end{equation}

The runtime of $\OddOneOut(S,w)$ is $O(dk)$.
So the runtime of $\OddOneOut(k,i)$ is $O(dkn^kv)$ and the runtime of $\OddOneOut(k)$ is $O(dkmn^kv)$.

Comparing the runtimes of $\OddOneOut(k)$ and $\topk(k)$, we can see that $\OddOneOut(k)$ is a factor of $O(m^{k-1}k^{-1})$ slower.
In real world applications, $m >\!\!> k$, and so $\OddOneOut$ will take considerably more time to compute.
In particular, the Mikolov test evaluations in Section \ref{sec:experiments} below use $m=50$ and $k=3$,
so the $\OddOneOut(k)$ score takes approximately 800x longer to compute.

To solve this problem, we use a sampling strategy.
Let $\tilde P$ denote the set of $p$ samples without replacement from the set $P$.
Then we rewrite Equation \ref{eq:OOOki} as
\begin{equation}
    \OddOneOut(k,i) = \frac 1 p \sum_{\mathclap{(S,w) \in \tilde P}} \OddOneOut(S,w)
\end{equation}
With this definition, the runtime of $\OddOneOut(k)$ is $O(dkmpv)$,
which is linear in all the parameters of interest.
\fixme{In our experiments, we found $p=1000$ to give sufficiently accurate results without taking too much computation.}

\section{Experiments}
\label{sec:experiments}

We demonstrate the usefulness of our evaluation metrics with two experiments.
First, we show that the $\OddOneOut$ and $\topk$ metrics are better measures of word embedding quality than the $\analogy$ metric in the low-resource regime.
Second, we show that the $\OddOneOut$ and $\topk$ metrics are useful for model selection in an emoji embedding task where the analogy task cannot be applied.
This experiment also demonstrates that the $\OddOneOut$ and $\topk$ metrics correlate with downstream task performance.

\subsection{English Experiments}

This experiment measures the performance of the $\OddOneOut$, $\topk$, and $\analogy$ metrics as a function of data set size.

%Figure \ref{fig:data_vs_methods} shows the results of the evaluation of these models on three intrinsic tasks-- \texttt{Analogy}, \texttt{OddOneOut}, and \texttt{Topk}.
 %We see that for the various tasks both \texttt{OddOneOut} and \texttt{Topk} start to see an increase in performance with roughly $32x$ less data compared to \texttt{Analogy}. 
 %In addition, though the shapes are roughly similar, the curve for \texttt{OddOneOut} and \texttt{Topk} have steeper slopes before plateauing, 
 %indicating a greater sensitivity to amount of data.
  %Overall, these findings suggest that in the low resource scenarios, the analogy task does will not always provide adequate intrinsic evaluation for word embeddings,
  %especially compared to these simpler methods.

For training data, we use a 2017 dump of the English-language Wikipedia 
that contains 2 billion total tokens and 2 million unique tokens.
The dataset is freely distributed with the popular gensim library \cite{rehurek_lrec} for training word embeddings,
and it is therefore widely used.
State-of-the-art embeddings are trained on significantly larger datasets---%
for example, datasets based on the common crawl contain hundreds of billions of tokens even for non-English languages \cite{buck2014n,grave2018learning}---%
but since our emphasis is on the low-resource setting,
this 2 billion token dataset is sufficient.

Using the wikipedia dataset, we generate a series of synthetic low-resource datasets of varying size.
First, we sort the articles in the wikipedia dataset randomly.%
\footnote{
    This sorting is required for our low-resource datasets to be representative of English language text.
    Without this random sorting step,
    most of our datasets would be based only on articles that begin with the letter \texttt{A},
    and therefore would not contain a representative sample of English words.
 %This is a problem because larger models would have the benefit of both greater amounts of data as well as a greater contextual variety of data compared to smaller ones. 
 %To mitigate this bias we use a linear congruential generator to randomly stream the training data in a way that is reproducible. 
 %Thus, in addition to mitigating model bias tied to the content of training data, our random generator also ensures that each model uses a subset of the training data of models larger than it.
}
Then, each dataset $i$ contains the first $2^i$ tokens in the randomly ordered wikipedia dump.

On each of these low-resource datasets,
we train a word2vec skipgram model with gensim's default hyperparameters\footnote{
Embedding dimension 100, number of epochs 1, learning rate 0.025, window size is 5, min count is 5.
\fixme{
    should we defined min count?
}
},
which are known to work well in many contexts.
Importantly, we do not tune these hyperparameters for each low-resource dataset.
Instead, we use the same hyperparameters because our goal is to isolate the effects of dataset size on the three evaluation metrics.

For the analogy task, we use the standard Google Analogy Test Set introduced by \citep{mikolov2013efficient}.
This test set contains 14 sets of analogies,
and each analogy set contains 2 categories that are being compared.
For example,
the countries-capitals analogy set which has analogies like
\begin{equation}
\texttt{England} - \texttt{London} + \texttt{Paris} \approx \texttt{France}
\end{equation}
contains both a list of countries and a list of capitals.
We generate test sets for the $\OddOneOut$ and $\topk$ tasks from the 28 categories in the Google test set.
%In doing so, we lose information about the relatedness between categories.
%The fact that the analogy task

The results are shown in Figure \ref{fig:size_vs_acc}.
\fixme{
The $\OddOneOut$ and $\topk$ metrics show higher results
}

\begin{figure*}
\centering
    \resizebox{6in}{3in}{\includegraphics{cross_cat_acc.pdf} }
\caption{
    The methods perform better on some categories than others. $\topk$  seems to excel in categories that are more homogenous like 'family female',
 while analogies seem to work best with geographical relationships.
  Note that in adapting the Google analogy set to work with our methods required splitting each relationship pair into two separate categories. 
  As a result the analogy score for a given relationship is shown twice;
   one bar in each of the categories that make up the pair.
   }
\label{fig:data_vs_methods}
\end{figure*}


\subsection{Emoji Experiments}
\begin{figure}
\centering
    \resizebox{\columnwidth}{!}{\includegraphics{emoji_exp.pdf} }
\caption{This figures shows the tuning of emoji embeddings across different learning learning rates where the max accuracy for each metric is marked with a point.
 Both $\topk$ and $\OddOneOut$ follow the shape of \cite{eisner2016emoji2vec} area under the curve and accuracy metrics.
 Our methods lead us to select essentially the same hyperparameters as \cite{eisner2016emoji2vec} and reproduced results on the downstream task.}
\label{fig:emoji}
\end{figure}

This second experiment demonstrates the versatility of our methods by applying them to the domain of emoji embeddings.
We show that our generic $\topk$ and $\OddOneOut$ metrics perform as well as a custom designed emoji evaluation metric,
but our metrics are more versatile.

Emoji embeddings are an important topic of study because they are used to improve the performance of sentiment analysis systems \cite[e.g.][]{eisner2016emoji2vec,felbo2017using,barbieri2017emojis,ai2017untangling,wijeratne2017semantics,al2019smile}. 
Unfortunately,
the standard analogy task is not suitable for evaluating the quality of emoji embeddings for two reasons.
First, emoji embeddings are inherently low-resource---%
only 3000 unique emojis exist in the Unicode standard---%
and thus evaluation techniques specifically designed for the low-resource setting will be more effective.
Second, the semantics of most emojis do not allow them to be used in any analogy task.
In particular, the original emoji2vec paper \cite{eisner2016emoji2vec} identifies only \fixme{5?} possible emoji analogies.

In order to tune their emoji embeddings, \citet{eisner2016emoji2vec} therefore do not use the analogy task,
and instead introduce an ad-hoc ``emoji-description classification'' metric that required the creation of a test set with manually labeled emotion-description pairs.
Due to the expense of manually creating this test set,
only \fixme{64?} of the 3000 Unicode emojis are included.
The $\topk$ and $\OddOneOut$ metrics improve on the ``emoji-description classification'' metric because they are able to evaluate the quality of all emojis and require no manual test set creation.
For our test set categories, we use the categories that the Unicode standard provides for each emoji.%
\footnote{For the full list of categories, see \url{https://unicode.org/Public/emoji/13.0/emoji-test.txt}}

To test the performance of the three metrics,
we use them to tune the hyperparameters of an emoji2vec model.
To ensure the fairest comparison possible, we use the original emoji2vec code for training and model selection,
changing only the function call to the metric used.
In particular, this means we are only embedding and evaluating on the subset of \fixme{64?} emojis supported by the ``emoji-description classification'' metric.
The code allows tuning the model's learning rate, \fixme{insert other options here}.
We found that the learning rate was the only hyperparameter to have a significant impact,
and Figure \ref{fig:emoji} shows how the learning rate effects the performance on the three evaluation metrics and the performance on a downstream sentiment analysis task.
All three metrics show optimal performance with a learning rate of approximately $8\times10^{-4}$,
which also results in the best performance on the downstream task.
This indicates that our $\topk$ and $\OddOneOut$ metrics generate the same models as the specialized ``emoji-description classification'' metric,
but our metrics have the advantage of being simpler, more widely applicable, and easier to generate test data for.

\fixme{Explain model selection and that comparison of metrics should not be done.}

\section{Multilingual Content Analysis}
\label{sec:mca}

In this section we perform the first highly multilingual analysis of word embeddings for low-resource languages.
We analyze 18 languages provided by the Classical Languages ToolKit (CLTK) library \citep{johnson2014}.%
\footnote{
    Specifically, we analyzed all languages for which CLTK provides both a training corpus and a tokenization function,
    as these are the minimum requirements needed for training word embeddings.
}
Each of these languages is ``extinct'' in the sense that no new native text will ever be generated in these languages.
To get better models on these languages, it is impossible to collect more data,
we must develop better techniques for the low-resource setting.
The largest of these language datasets is Ancient Greek, with 37.8 million tokens,
and the smallest is Ancient Gujarati with only 1813 tokens.

First we describe a procedure for automatically generating test set data for the $\OddOneOut$ and $\topk$ tasks using wikidata.
Then, we describe our model training and selection procedure for each language.
Finally, we perform the LCT task and perform an interlanguage analysis of the corpora's content.

\subsection{Test Set Generation with Wikidata}

One of the most difficult and time consuming steps in the process of generating of high quality word embeddings is the creation of a comprehensive test set.
 Consequently, one of the biggest advantages of the $\OddOneOut$ and $\topk$ methods is that compatible test sets can be semi-automatically generated in hundreds of languages.
This is possible using Wikidata. 
This publicly available SPARQL database contains a comprehensive structure of the semantic content contained in Wikipedia
along with its relationship to other items.
Simple queries constructed using the Wikidata Query Service can be used to return semantic categories of words that can be included in a test set.

Using Wikidata we were able to reconstruct all of the semantic categories contained within the Google analogy set with the option of greater customization and more categories.
Additionally, Wikidata supports the translation of queried items into 461 languages, allowing test sets to easily be converted between languages.
Though not all items have translated labels in all 461 languages, support is quite extensive and will only get better. 
\fixme{Is 461 correct?}

One of the disadvantages of using Wikidata is that syntactic categories are much harder to construct;
however, since semantic categories are usually more difficult to generate 
and require more arbitrary decisions this approach is still very valuable.

\fixme{What are the exact categories used?}

\fixme{Does the library allow the easy construction of these categories?}

\subsection{Model Selection}

% Improved Table - Combined Score
\begin{table*}[t]
\centering
%\scalebox{.54}{
\tiny
\begin{tabular}{l|llrr|llrrrrl|rrr}
\toprule
\multicolumn{5}{c}{\bf Corpus} & \multicolumn{7}{c}{\bf Parameters} & \multicolumn{3}{c}{\bf Scores} \\
\midrule
& Language & Test Set & Tokens & Unique Tokens & Model & Type & Dim & Window & LR & Min Count & Lemma & $\OddOneOut$ & $\topk$ & \texttt{Combined} \\
\midrule
    
    \multirow{2}{*}{Hellenic}& greek & ancient greek & \num{37868209} & \num{1877574} & w2v & cbow & 40 & 9 & -1 & 4 & False & 1140 & 8 & 17.86\\
    & greek & modern greek & \num{37868209} & \num{1877574} & fast & sg & 90 & 7 & -1 & 5 & True & 300 & 9 & 19.36\\  
    \midrule
    \multirow{2}{*}{Italic}&latin & latin & \num{17777429} & \num{470790} & w2v & cbow & 50 & 10 & -1 & 7 & False & 2748 & 48 & 96.28\\ 
    &old french  & french & \num{68741} & \num{8343} &  fast & sg & 250 & 6 & -1 & 8  & False & 1 & 7 & 3.20\\
    \midrule
    \multirow{5}{*}{Germanic}&middle english & english (old) & \num{7048144} & \num{314527} & fast & sg & 90 & 5 & -1 & 7 & False & 239 & 7 & 15.48 \\
    &middle high german & german & \num{2090954} & \num{60674} & fast & cbow & 15 & 6 & -1 & 3 & False & 9 & 19 & 13.33 \\
    &old english  & english (old) & \num{104011} & \num{33018} & fast & cbow & 425 & 3 & -1 & 3 & True & 0 & 1  & 1.33 \\
    &old norse & icelandic & \num{458377} & \num{59186} & w2v & cbow & 60 & 10 & -1 & 3 & False & 968 & 2 & 5.98 \\
    &old swedish & swedish & \num{1297740} & \num{116374} &  fast & sg & 50 & 8 & -1 & 5 & False & 50 & 1 & 3.85 \\
    \midrule
    \multirow{8}{*}{Indo-Aryan}&bengali & bengali & \num{5539} & \num{2323} & fast & cbow & 15 & 3 & -1 & 4 & False & 0 & 2 & 1.50 \\
    &gujarati & gujarati & \num{1813} & \num{1140} & fast & sg & 80 & 3 & -1 & 5 & False & 0 & 7 & 1.78 \\
    &hindi & hindi& \num{587655} & \num{55483} & fast & cbow & 45 & 4 & -1 & 8 & False & 263 & 2 & 5.93\\
    &malayalam & malayalam& \num{9235} & \num{5405} & - & - & - & - & - & - & - & - & - & - \\
    &marathi & marathi & \num{797926} & \num{96778} & w2v & sg & 400 & 4 & -1 & 6 & False & 342 & 1 & 3.98 \\
    &punjabi & punjabi & \num{1024075} & \num{31343} & fast & sg & 50 & 8 & -1 & 5 & False & 0 & 1 & 1.3 \\
    &sanskrit & sanskrit & \num{4042204} & \num{896480} & w2v & sg & 35 & 9 & -1 & 10 & False & 1530 & 1 & 3.99\\
    &telugu & telugu & \num{537673} & \num{276330} &  w2v & cbow & 60 & 10 & -1 & 3 & False & 50 & 0 & 1.96 \\
    \midrule
    \multirow{2}{*}{Semitic}&classical arabic & arabic& \num{81306} & \num{20493} & - & - & - & - & - & - & - & - & - & - \\
    &hebrew & hebrew & \num{41378460} & \num{893512} & fast & sg & 30 & 4 & -1 & 3 & False & 1098 & 6 & 13.91 \\  
\bottomrule
\end{tabular}
%}
\caption{The table above provides details for the best model trained for languages supported by CLTK.
Following a tuning process, models were chosen by their $\texttt{Combined Score}$
which is calculated as the harmonic mean of $\topk$ and $\OddOneOut$.
It is important to note that the absolute score for our evaluation metrics are not important in and of themselves,
rather they are important as indicators of a change in embedding quality that the analogy task would fail to show.
To emphasize this, we have reported the raw number of correct answers for each metric.
Using $\OddOneOut$ and $\topk$ allows us to tune models trained on corpora with unique token counts in the thousands instead of the millions.}
\label{table:language}
\end{table*}

There are 7 hyperparameters for our language models,
and we use the random search method \citep{bergstra2012random} to tune these hyperparameters.
Random search is simple to implement, computationally efficient, easy to parallelize, and avoids the curse of dimensionality inherent to grid search and Bayesian optimization methods.
\fixme{
Table \ref{table:hyperparam} defines the hyperparameters and the range of values sampled for each.
Should this be a table or included directly in the text?
\begin{table}
    \includegraphics[width=\columnwidth]{example-image-a}
    \caption{Hyperparameters.}
    \label{table:hyperparam}
\end{table}
}

To ensure a fair comparison,
we randomly sampled 100 sets of hyperparameter combinations.%
\footnote{We found that 100 combinations was sufficient to give good results without being too computationally burdensome.}
Then train each language on this same set of hyperparameters.
The best results are reported in Table \ref{table:language}.
The optimal set of hyperparameters is different for each language,
which underscores the importance of proper model tuning in the low-resource regime.
Whereas all previous highly multilingual work used the same set of hyperparemeters for all models,
we show that this is not an optimal strategy.

\subsection{Language Comparison Task}

We now introduce a novel downstream task for word embeddings called the \emph{Language Comparison Task} (LCT).
The goal of this task is to better visualize the topics written about in our 18 language corpora.
We choose three religious topics: Biblical Figures, Facets of Buddhism, and Facets of Hinduism.
Each of these topics has an entry in Wikidata,
\fixme{I'm not sure what the correct wikidata terminology is here.}
and so it is easy to create categories based on these topics in each language.
Figure \ref{fig:downstream} provides a list of words associated with each topic.

For each language and topic, we compute the $\topk$ and $\OddOneOut$ metrics and plot the results in Figure \ref{fig:downstream}.
\fixme{explain significance}

%With intentionally selected categories we are able draw out the differences in content between the different language models
%that in many cases matches our intuition. 
%By evaluating on a category of biblical figures we see highest performance from the Hebrew and Latin models followed by other European languages; 
%whereas Facets of Buddhism and Facets of Hinduism see high performance from the Indic language models.

Perhaps more interesting, this downstream task also gives rise to circumstances in which $\topk$ performance exceeded $\OddOneOut$. 
In some cases (like Middle High German on Biblical Figures) $\topk$ merely surpasses $\OddOneOut$ 
and in others (such as Gujarati on Facets of Hinduism) it is the only metric to achieve a performance score. 
It is likely that a complex interaction between the model and the category being evaluated on can result in either method achieving better performance, 
thus it is valuable to have both methods at our disposal. 
Since this exact relationship is still not fully understood we tune and evaluate models using both metrics 
and compute the harmonic mean to choose the most accurate model.

\fixme{
    There's a simpler way to measure how much each corpus discusses each category:
    simply count the number of times each word from the category is used.
    Can we explain why our method is better than this simpler method?
}

\begin{figure*}
\centering
\resizebox{6in}{2in}{\includegraphics{biblical-raw.pdf}}
\resizebox{6in}{2in}{\includegraphics{buddhism-raw.pdf}}
\resizebox{6in}{2in}{\includegraphics{hinduism-raw.pdf}}

\caption{
    (\textbf{Top})
    High performance on the Biblical Figures category indicates some level of biblical influence via the corpus. 
    Interestingly, we see that greek embeddings optimized on the Modern Greek test set significantly outperformed the embeddings optimized for the Ancient Greek. 
    This matches our intuition that things of a biblical nature have had a greater influence on Modern Greek than Ancient Greek.
    (\textbf{Middle})
    Though many of the Indic language embeddings performed well on the Facets of Buddhism, 
    surprisingly so did our Latin and Hebrew embeddings. 
    This leads us to believe that some Buddhist concepts and words are shared by corpora spanning languages as diverse as Hebrew, Latin, and Hindi. 
    At the same time, it should also be noted that Latin and Hebrew were two of the largest models trained compared to other classical languages and thus also likely benefit from greater resource richness.
    (\textbf{Bottom})
    Similar to Figure \ref{fig:biblical} we see languages more closely related to the topic of the category achieving better performance, in this case primarily the Indic languages. 
    Interstingly, our Bengali embeddings performed poorly on this category suggesting that the corpus did not refer heavily to the Hindu context.
}
\label{fig:downstream}
\end{figure*}

%animals
%\begin{figure*}
%\centering
%\includegraphics{animals-raw.pdf} 
%\caption{}
%\label{fig:multilingual}
%\end{figure*}

\section{Discussion}
\label{sec:discussion}

\fixme{
Don't actually make any changes here.
The notes in this section are just for me.
}

\fixme{
    \citet{michel-etal-2020-exploring} study Hiligaynon
}

Not yet using wikidata to its fullest extent.
Wikidata categories form a hierarchy.
More accurate evaluations could be done by using all categories in the hierarchy,
weighting top-level categories higher than leaf categories.
This has computational challenges and so we reserve this for future work.

\fixme{Where should this go?
\citet{gonen2020simple} propose a new method for comparing the similarity of two word embeddings in order to find words that are used differently.
Their method assumes that quality word embeddings have already been trained,
and our evaluation metrics provide a way to ensure that this training is done well.
}

\fixme{Where should this go?
\citet{camacho-collados-navigli-2016-find} propose the \texttt{OutlierDetection} metric that is superficially similar to our $\OddOneOut$ metric.
Differences include: their metric is designed to evaluate only a single category relative to itself and is therefore relatively efficient to compute.
Our metric is designed to evaluate many categories relative to all words in the training data.
A naive computation is therefore much more expensive to compute,
and so we introduce a sampling strategy to make the metric computationally feasible.
We also demonstrate that our metric is suitable for the low-resource regime (they only evaluate on datasets with billions of tokens) and make our method available in an easy-to-use open source python library.
}

\fixme{Add this?
Recent work \citet{guntuku2019studying} combines uses the subdivision technique (Section \ref{}) to study how emoji are used differently in different parts of the world.
This technique turns high-resource tasks into low-resource tasks,
and so if you start with a low-resource task it will become even more low-resource.
}

\fixme{
Where does this go?

\citet{al2013polyglot} trained word2vec embeddings on 100 different languages using Wikipedia as the training data.
\citet{grave2018learning} extended this work by training FastText embeddings on 157 languages using data from the Common Crawl project.
\begin{enumerate}
\item
For both papers, the Hindi language had the smallest amount of training data,
with 23 million and 1.8 billion tokens, respectively.
In Section \ref{sec:}, we consider languages with significantly less training data.
For example, the Ancient Hindi language contains only 0.6 million tokens,
and we are still able to generate meaningful word embeddings using our evaluation metrics.

\item
Due to the difficulty of evaluating so many languages, however, the authors evaluate the embeddings only on 10 languages,
and the other 147 remain unevaluated.
\end{enumerate}
}


\fixme{where does this go?
Bias-variance tradeoff in dimensionality of word embeddings \cite{yin2018dimensionality}
}


%\section{Related Work}
%\label{sec:related}



%\subsection{Nate}
%
%Look at the way these papers run experiments:
%
%\cite{tifrea2018poincar,meng2019spherical}
%
%Datasets at: \url{https://aclweb.org/aclwiki/Similarity_(State_of_the_art)}
%
%Use wikidata to automatically generate classes: https://www.wikidata.org/wiki/Q43689 
%https://pywikidata.readthedocs.io/en/latest/
%
%wikidata church fathers in latin: \url{https://query.wikidata.org/#SELECT%20%3Fperson%20%3FpersonLabel%0AWHERE%20%7B%0A%20%20%3Fperson%20wdt%3AP361%20wd%3AQ182603.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22la%22.%20%7D%0A%7D}
%
%SPARQL Tutorial: \url{https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial}
%
%Can we use wikidata to automatically generate good evaluations?
%
%\section{Discussion}
%\label{sec:discussion}



%%% End of my stuff 


%\section{Electronically-available resources}
%
%ACL provides this description and accompanying style files at
%\begin{quote}
%\url{https://2020.emnlp.org/files/emnlp2020-templates.zip}
%\end{quote}
%We strongly recommend the use of these style files, which have been appropriately tailored for the EMNLP 2020 proceedings.
%
%\paragraph{\LaTeX-specific details:}
%The templates include the \LaTeX2e{} source (\texttt{\small emnlp2020.tex}),
%the \LaTeX2e{} style file used to format it (\texttt{\small emnlp2020.sty}),
%an ACL bibliography style (\texttt{\small acl\_natbib.bst}),
%an example bibliography (\texttt{\small emnlp2020.bib}),
%and the bibliography for the ACL Anthology (\texttt{\small anthology.bib}).
%
%
%\section{Length of Submission}
%\label{sec:length}
%
%The conference accepts submissions of long papers and short papers.
%Long papers may consist of up to eight (8) pages of content plus unlimited pages for references.
%Upon acceptance, final versions of long papers will be given one additional page -- up to nine (9) pages of content plus unlimited pages for references -- so that reviewers' comments can be taken into account.
%Short papers may consist of up to four (4) pages of content, plus unlimited pages for references.
%Upon acceptance, short papers will be given five (5) pages in the proceedings and unlimited pages for references. 
%For both long and short papers, all illustrations and tables that are part of the main text must be accommodated within these page limits, observing the formatting instructions given in the present document.
%Papers that do not conform to the specified length and formatting requirements are subject to be rejected without review.
%
%The conference encourages the submission of additional material that is relevant to the reviewers but not an integral part of the paper.
%There are two such types of material: appendices, which can be read, and non-readable supplementary materials, often data or code.
%Additional material must be submitted as separate files, and must adhere to the same anonymity guidelines as the main paper.
%The paper must be self-contained: it is optional for reviewers to look at the supplementary material.
%Papers should not refer, for further detail, to documents, code or data resources that are not available to the reviewers.
%Refer to Appendices~\ref{sec:appendix} and \ref{sec:supplemental} for further information. 
%
%Workshop chairs may have different rules for allowed length and whether supplemental material is welcome.
%As always, the respective call for papers is the authoritative source.
%
%
%\section{Anonymity}
%As reviewing will be double-blind, papers submitted for review should not include any author information (such as names or affiliations). Furthermore, self-references that reveal the author's identity, \emph{e.g.},
%\begin{quote}
%We previously showed \citep{Gusfield:97} \ldots
%\end{quote}
%should be avoided. Instead, use citations such as 
%\begin{quote}
%\citet{Gusfield:97} previously showed\ldots
%\end{quote}
%Please do not use anonymous citations and do not include acknowledgements.
%\textbf{Papers that do not conform to these requirements may be rejected without review.}
%
%Any preliminary non-archival versions of submitted papers should be listed in the submission form but not in the review version of the paper.
%Reviewers are generally aware that authors may present preliminary versions of their work in other venues, but will not be provided the list of previous presentations from the submission form.
%
%Once a paper has been accepted to the conference, the camera-ready version of the paper should include the author's names and affiliations, and is allowed to use self-references.
%
%\paragraph{\LaTeX-specific details:}
%For an anonymized submission, ensure that {\small\verb|\aclfinalcopy|} at the top of this document is commented out, and that you have filled in the paper ID number (assigned during the submission process on softconf) where {\small\verb|***|} appears in the {\small\verb|\def\aclpaperid{***}|} definition at the top of this document.
%For a camera-ready submission, ensure that {\small\verb|\aclfinalcopy|} at the top of this document is not commented out.
%
%
%\section{Multiple Submission Policy}
%
%EMNLP 2020 will not consider any paper that is under review in a journal or another conference at the time of submission, and submitted papers must not be submitted elsewhere during the EMNLP 2020 review period. This policy covers all refereed and archival conferences and workshops (e.g., COLING, NeurIPS, ACL workshops). For example, a paper under review at an ACL workshop cannot be dual-submitted to EMNLP 2020. The only exception is that a paper can be dual-submitted to both EMNLP 2020 and an EMNLP workshop which has its submission deadline falling after our original notification date of August 8, 2020. In addition, we will not consider any paper that overlaps significantly in content or results with papers that will be (or have been) published elsewhere. 
%
%Authors submitting more than one paper to EMNLP 2020 must ensure that their submissions do not overlap significantly ($>25$\%) with each other in content or results.
%
%\section{Formatting Instructions}
%
%Manuscripts must be in two-column format.
%Exceptions to the two-column format include the title, authors' names and complete addresses, which must be centered at the top of the first page, and any full-width figures or tables (see the guidelines in Section~\ref{ssec:title-authors}).
%\textbf{Type single-spaced.}
%Start all pages directly under the top margin.
%The manuscript should be printed single-sided and its length should not exceed the maximum page limit described in Section~\ref{sec:length}.
%Pages should be numbered in the version submitted for review, but \textbf{pages should not be numbered in the camera-ready version}.
%
%\paragraph{\LaTeX-specific details:}
%The style files will generate page numbers when {\small\verb|\aclfinalcopy|} is commented out, and remove them otherwise.
%
%
%\subsection{File Format}
%\label{sect:pdf}
%
%For the production of the electronic manuscript you must use Adobe's Portable Document Format (PDF).
%Please make sure that your PDF file includes all the necessary fonts (especially tree diagrams, symbols, and fonts with Asian characters).
%When you print or create the PDF file, there is usually an option in your printer setup to include none, all or just non-standard fonts.
%Please make sure that you select the option of including ALL the fonts.
%\textbf{Before sending it, test your PDF by printing it from a computer different from the one where it was created.}
%Moreover, some word processors may generate very large PDF files, where each page is rendered as an image.
%Such images may reproduce poorly.
%In this case, try alternative ways to obtain the PDF.
%One way on some systems is to install a driver for a postscript printer, send your document to the printer specifying ``Output to a file'', then convert the file to PDF.
%
%It is of utmost importance to specify the \textbf{A4 format} (21 cm x 29.7 cm) when formatting the paper.
%Print-outs of the PDF file on A4 paper should be identical to the hardcopy version.
%If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chairs as soon as possible.
%
%\paragraph{\LaTeX-specific details:}
%PDF files are usually produced from \LaTeX{} using the \texttt{\small pdflatex} command.
%If your version of \LaTeX{} produces Postscript files, \texttt{\small ps2pdf} or \texttt{\small dvipdf} can convert these to PDF.
%To ensure A4 format in \LaTeX, use the command {\small\verb|\special{papersize=210mm,297mm}|}
%in the \LaTeX{} preamble (below the {\small\verb|\usepackage|} commands) and use \texttt{\small dvipdf} and/or \texttt{\small pdflatex}; or specify \texttt{\small -t a4} when working with \texttt{\small dvips}.
%
%\subsection{Layout}
%\label{ssec:layout}
%
%Format manuscripts two columns to a page, in the manner these
%instructions are formatted.
%The exact dimensions for a page on A4 paper are:
%
%\begin{itemize}
%\item Left and right margins: 2.5 cm
%\item Top margin: 2.5 cm
%\item Bottom margin: 2.5 cm
%\item Column width: 7.7 cm
%\item Column height: 24.7 cm
%\item Gap between columns: 0.6 cm
%\end{itemize}
%
%\noindent Papers should not be submitted on any other paper size.
%If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chairs above as soon as possible.
%
%\subsection{Fonts}
%
%For reasons of uniformity, Adobe's \textbf{Times Roman} font should be used.
%If Times Roman is unavailable, you may use Times New Roman or \textbf{Computer Modern Roman}.
%
%Table~\ref{font-table} specifies what font sizes and styles must be used for each type of text in the manuscript.
%
%\begin{table}
%\centering
%\begin{tabular}{lrl}
%\hline \textbf{Type of Text} & \textbf{Font Size} & \textbf{Style} \\ \hline
%paper title & 15 pt & bold \\
%author names & 12 pt & bold \\
%author affiliation & 12 pt & \\
%the word ``Abstract'' & 12 pt & bold \\
%section titles & 12 pt & bold \\
%subsection titles & 11 pt & bold \\
%document text & 11 pt  &\\
%captions & 10 pt & \\
%abstract text & 10 pt & \\
%bibliography & 10 pt & \\
%footnotes & 9 pt & \\
%\hline
%\end{tabular}
%\caption{\label{font-table} Font guide. }
%\end{table}
%
%\paragraph{\LaTeX-specific details:}
%To use Times Roman in \LaTeX2e{}, put the following in the preamble:
%\begin{quote}
%\small
%\begin{verbatim}
%\usepackage{times}
%\usepackage{latexsym}
%\end{verbatim}
%\end{quote}
%
%
%\subsection{Ruler}
%A printed ruler (line numbers in the left and right margins of the article) should be presented in the version submitted for review, so that reviewers may comment on particular lines in the paper without circumlocution.
%The presence or absence of the ruler should not change the appearance of any other content on the page.
%The camera ready copy should not contain a ruler.
%
%\paragraph{Reviewers:}
%note that the ruler measurements may not align well with lines in the paper -- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
%In most cases one would expect that the approximate location will be adequate, although you can also use fractional references (\emph{e.g.}, this line ends at mark $295.5$).
%
%\paragraph{\LaTeX-specific details:}
%The style files will generate the ruler when {\small\verb|\aclfinalcopy|} is commented out, and remove it otherwise.
%
%\subsection{Title and Authors}
%\label{ssec:title-authors}
%
%Center the title, author's name(s) and affiliation(s) across both columns.
%Do not use footnotes for affiliations.
%Place the title centered at the top of the first page, in a 15-point bold font.
%Long titles should be typed on two lines without a blank line intervening.
%Put the title 2.5 cm from the top of the page, followed by a blank line, then the author's names(s), and the affiliation on the following line.
%Do not use only initials for given names (middle initials are allowed).
%Do not format surnames in all capitals (\emph{e.g.}, use ``Mitchell'' not ``MITCHELL'').
%Do not format title and section headings in all capitals except for proper names (such as ``BLEU'') that are
%conventionally in all capitals.
%The affiliation should contain the author's complete address, and if possible, an electronic mail address.
%
%The title, author names and addresses should be completely identical to those entered to the electronical paper submission website in order to maintain the consistency of author information among all publications of the conference.
%If they are different, the publication chairs may resolve the difference without consulting with you; so it is in your own interest to double-check that the information is consistent.
%
%Start the body of the first page 7.5 cm from the top of the page.
%\textbf{Even in the anonymous version of the paper, you should maintain space for names and addresses so that they will fit in the final (accepted) version.}
%
%
%\subsection{Abstract}
%Use two-column format when you begin the abstract.
%Type the abstract at the beginning of the first column.
%The width of the abstract text should be smaller than the
%width of the columns for the text in the body of the paper by 0.6 cm on each side.
%Center the word \textbf{Abstract} in a 12 point bold font above the body of the abstract.
%The abstract should be a concise summary of the general thesis and conclusions of the paper.
%It should be no longer than 200 words.
%The abstract text should be in 10 point font.
%
%\subsection{Text}
%Begin typing the main body of the text immediately after the abstract, observing the two-column format as shown in the present document.
%
%Indent 0.4 cm when starting a new paragraph.
%
%\subsection{Sections}
%
%Format section and subsection headings in the style shown on the present document.
%Use numbered sections (Arabic numerals) to facilitate cross references.
%Number subsections with the section number and the subsection number separated by a dot, in Arabic numerals.
%
%\subsection{Footnotes}
%Put footnotes at the bottom of the page and use 9 point font.
%They may be numbered or referred to by asterisks or other symbols.\footnote{This is how a footnote should appear.}
%Footnotes should be separated from the text by a line.\footnote{Note the line separating the footnotes from the text.}
%
%\subsection{Graphics}
%
%Place figures, tables, and photographs in the paper near where they are first discussed, rather than at the end, if possible.
%Wide illustrations may run across both columns.
%Color is allowed, but adhere to Section~\ref{ssec:accessibility}'s guidelines on accessibility.
%
%\paragraph{Captions:}
%Provide a caption for every illustration; number each one sequentially in the form:
%``Figure 1. Caption of the Figure.''
%``Table 1. Caption of the Table.''
%Type the captions of the figures and tables below the body, using 10 point text.
%Captions should be placed below illustrations.
%Captions that are one line are centered (see Table~\ref{font-table}).
%Captions longer than one line are left-aligned (see Table~\ref{tab:accents}).
%
%\begin{table}
%\centering
%\begin{tabular}{lc}
%\hline
%\textbf{Command} & \textbf{Output}\\
%\hline
%\verb|{\"a}| & {\"a} \\
%\verb|{\^e}| & {\^e} \\
%\verb|{\`i}| & {\`i} \\ 
%\verb|{\.I}| & {\.I} \\ 
%\verb|{\o}| & {\o} \\
%\verb|{\'u}| & {\'u}  \\ 
%\verb|{\aa}| & {\aa}  \\\hline
%\end{tabular}
%\begin{tabular}{lc}
%\hline
%\textbf{Command} & \textbf{Output}\\
%\hline
%\verb|{\c c}| & {\c c} \\ 
%\verb|{\u g}| & {\u g} \\ 
%\verb|{\l}| & {\l} \\ 
%\verb|{\~n}| & {\~n} \\ 
%\verb|{\H o}| & {\H o} \\ 
%\verb|{\v r}| & {\v r} \\ 
%\verb|{\ss}| & {\ss} \\
%\hline
%\end{tabular}
%\caption{Example commands for accented characters, to be used in, \emph{e.g.}, \BibTeX\ names.}\label{tab:accents}
%\end{table}
%
%\paragraph{\LaTeX-specific details:}
%The style files are compatible with the caption and subcaption packages; do not add optional arguments.
%\textbf{Do not override the default caption sizes.}
%
%
%\subsection{Hyperlinks}
%Within-document and external hyperlinks are indicated with Dark Blue text, Color Hex \#000099.
%
%\subsection{Citations}
%Citations within the text appear in parentheses as~\citep{Gusfield:97} or, if the author's name appears in the text itself, as \citet{Gusfield:97}.
%Append lowercase letters to the year in cases of ambiguities.  
%Treat double authors as in~\citep{Aho:72}, but write as in~\citep{Chandra:81} when more than two authors are involved. Collapse multiple citations as in~\citep{Gusfield:97,Aho:72}. 
%
%Refrain from using full citations as sentence constituents.
%Instead of
%\begin{quote}
%  ``\citep{Gusfield:97} showed that ...''
%\end{quote}
%write
%\begin{quote}
%``\citet{Gusfield:97} showed that ...''
%\end{quote}
%
%\begin{table*}
%\centering
%\begin{tabular}{lll}
%\hline
%\textbf{Output} & \textbf{natbib command} & \textbf{Old ACL-style command}\\
%\hline
%\citep{Gusfield:97} & \small\verb|\citep| & \small\verb|\cite| \\
%\citealp{Gusfield:97} & \small\verb|\citealp| & no equivalent \\
%\citet{Gusfield:97} & \small\verb|\citet| & \small\verb|\newcite| \\
%\citeyearpar{Gusfield:97} & \small\verb|\citeyearpar| & \small\verb|\shortcite| \\
%\hline
%\end{tabular}
%\caption{\label{citation-guide}
%Citation commands supported by the style file.
%The style is based on the natbib package and supports all natbib citation commands.
%It also supports commands defined in previous ACL style files for compatibility.
%}
%\end{table*}
%
%\paragraph{\LaTeX-specific details:}
%Table~\ref{citation-guide} shows the syntax supported by the style files.
%We encourage you to use the natbib styles.
%You can use the command {\small\verb|\citet|} (cite in text) to get ``author (year)'' citations as in \citet{Gusfield:97}.
%You can use the command {\small\verb|\citep|} (cite in parentheses) to get ``(author, year)'' citations as in \citep{Gusfield:97}.
%You can use the command {\small\verb|\citealp|} (alternative cite without  parentheses) to get ``author year'' citations (which is useful for  using citations within parentheses, as in \citealp{Gusfield:97}).
%
%
%\subsection{References}
%Gather the full set of references together under the heading \textbf{References}; place the section before any Appendices. 
%Arrange the references alphabetically by first author, rather than by order of occurrence in the text.
%
%Provide as complete a citation as possible, using a consistent format, such as the one for \emph{Computational Linguistics\/} or the one in the  \emph{Publication Manual of the American 
%Psychological Association\/}~\citep{APA:83}.
%Use full names for authors, not just initials.
%
%Submissions should accurately reference prior and related work, including code and data.
%If a piece of prior work appeared in multiple venues, the version that appeared in a refereed, archival venue should be referenced.
%If multiple versions of a piece of prior work exist, the one used by the authors should be referenced.
%Authors should not rely on automated citation indices to provide accurate references for prior and related work.
%
%The following text cites various types of articles so that the references section of the present document will include them.
%\begin{itemize}
%\item Example article in journal: \citep{Ando2005}.
%\item Example article in proceedings, with location: \citep{borschinger-johnson-2011-particle}.
%\item Example article in proceedings, without location: \citep{andrew2007scalable}.
%\item Example arxiv paper: \citep{rasooli-tetrault-2015}. 
%\end{itemize}
%
%
%\paragraph{\LaTeX-specific details:}
%The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
%If your own bib file is named \texttt{\small emnlp2020.bib}, then placing the following before any appendices in your \LaTeX{}  file will generate the references section for you:
%\begin{quote}\small
%\verb|\bibliographystyle{acl_natbib}|\\
%\verb|\bibliography{emnlp2020}|
%\end{quote}
%
%You can obtain the complete ACL Anthology as a Bib\TeX\ file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
%To include both the anthology and your own bib file, use the following instead of the above.
%\begin{quote}\small
%\verb|\bibliographystyle{acl_natbib}|\\
%\verb|\bibliography{anthology,emnlp2020}|
%\end{quote}
%
%
%\subsection{Digital Object Identifiers}
%As part of our work to make ACL materials more widely used and cited outside of our discipline, ACL has registered as a CrossRef member, as a registrant of Digital Object Identifiers (DOIs), the standard for registering permanent URNs for referencing scholarly materials.
%
%All camera-ready references are required to contain the appropriate DOIs (or as a second resort, the hyperlinked ACL Anthology Identifier) to all cited works.
%Appropriate records should be found for most materials in the current ACL Anthology at \url{http://aclanthology.info/}.
%As examples, we cite \citep{goodman-etal-2016-noise} to show you how papers with a DOI will appear in the bibliography.
%We cite \citep{harper-2014-learning} to show how papers without a DOI but with an ACL Anthology Identifier will appear in the bibliography.
%
%\paragraph{\LaTeX-specific details:}
%Please ensure that you use Bib\TeX\ records that contain DOI or URLs for any of the ACL materials that you reference.
%If the Bib\TeX{} file contains DOI fields, the paper title in the references section will appear as a hyperlink to the DOI, using the hyperref \LaTeX{} package.
%
%
%\subsection{Appendices}
%Appendices, if any, directly follow the text and the
%references (but only in the camera-ready; see Appendix~\ref{sec:appendix}).
%Letter them in sequence and provide an informative title:
%\textbf{Appendix A. Title of Appendix}.
%
%\section{Accessibility}
%\label{ssec:accessibility}
%
%In an effort to accommodate people who are color-blind (as well as those printing to paper), grayscale readability is strongly encouraged.
%Color is not forbidden, but authors should ensure that tables and figures do not rely solely on color to convey critical distinctions.
%A simple criterion:
%All curves and points in your figures should be clearly distinguishable without color.
%
%\section{Translation of non-English Terms}
%
%It is also advised to supplement non-English characters and terms with appropriate transliterations and/or translations since not all readers understand all such characters and terms.
%Inline transliteration or translation can be represented in the order of:
%\begin{center}
%\begin{tabular}{c}
%original-form \\
%transliteration \\
%``translation''
%\end{tabular}
%\end{center}
%
%\section{\LaTeX{} Compilation Issues}
%You may encounter the following error during compilation: 
%\begin{quote}
%{\small\verb|\pdfendlink|} ended up in different nesting level than {\small\verb|\pdfstartlink|}.
%\end{quote}
%This happens when \texttt{\small pdflatex} is used and a citation splits across a page boundary.
%To fix this, the style file contains a patch consisting of two lines:
%(1) {\small\verb|\RequirePackage{etoolbox}|} (line 455 in \texttt{\small emnlp2020.sty}), and
%(2) A long line below (line 456 in \texttt{\small emnlp2020.sty}).
%
%If you still encounter compilation issues even with the patch enabled, disable the patch by commenting the two lines, and then disable the \texttt{\small hyperref} package by loading the style file with the \texttt{\small nohyperref} option:
%
%\noindent
%{\small\verb|\usepackage[nohyperref]{emnlp2020}|}
%
%\noindent
%Then recompile, find the problematic citation, and rewrite the sentence containing the citation. (See, {\em e.g.}, \url{http://tug.org/errors.html})
%
%\section*{Acknowledgments}
%
%The acknowledgments should go immediately before the references. Do not number the acknowledgments section.
%Do not include this section when submitting your paper for review.

\clearpage
%\nocite{*}
\bibliographystyle{acl_natbib}
%\interlinepenalty=10000
\bibliography{emnlp2020}


\end{document}
