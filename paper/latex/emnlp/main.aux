\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:data_vs_methods}{{1}{1}{The plot above compares the regions of effectiveness for our evaluation metrics. The analogy task fails to measure change in accuracy of the embeddings until the number of unique words in the training dataset reaches $2^{16}$, much later than both $\OddOneOut $ and $\topk $. Though $\OddOneOut $ seems the clear victor of these methods, experimentation in \ref {sec:experiments} shows that $\topk $ works better in some circumstances.\relax }{figure.caption.1}{}}
\newlabel{sec:method}{{2}{2}{Evaluation Methods}{section.2}{}}
\newlabel{eq:P}{{7}{2}{The $\OddOneOut $ method}{equation.2.7}{}}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\newlabel{sec:experiments}{{3}{3}{Experiments}{section.3}{}}
\newlabel{fig:data_vs_methods}{{2}{4}{The methods perform better on some categories than others. $\topk $ seems to excel in categories that are more homogenous like 'family female', while analogies seem to work best with geographical relationships. Note that in adapting the Google analogy set to work with our methods required splitting each relationship pair into two separate categories. As a result the analogy score for a given relationship is shown twice; one bar in each of the categories that make up the pair.\relax }{figure.caption.2}{}}
\citation{eisner2016emoji2vec}
\citation{eisner2016emoji2vec}
\citation{johnson2014}
\newlabel{fig:emoji}{{3}{5}{This figures shows the tuning of emoji embeddings across different learning learning rates where the max accuracy for each metric is marked with a point. Both $\topk $ and $\OddOneOut $ follow the shape of \cite {eisner2016emoji2vec} area under the curve and accuracy metrics. Our methods lead us to select essentially the same hyperparameters as \cite {eisner2016emoji2vec} and reproduced results on the downstream task.\relax }{figure.caption.3}{}}
\newlabel{table:language}{{1}{6}{The table above provides details for the best model trained for languages supported by CLTK. Following a tuning process, models were chosen by their $\texttt {Combined Score}$ which is calculated as the harmonic mean of $\topk $ and $\OddOneOut $. It is important to note that the absolute score for our evaluation metrics are not important in and of themselves, rather they are important as indicators of a change in embedding quality that the analogy task would fail to show. To emphasize this, we have reported the raw number of correct answers for each metric. Using $\OddOneOut $ and $\topk $ allows us to tune models trained on corpora with unique token counts in the thousands instead of the millions.\relax }{table.caption.4}{}}
\citation{*}
\bibstyle{acl_natbib}
\bibdata{emnlp2020}
\bibcite{agrawal2018learning}{{1}{2018}{{Agrawal et~al.}}{{Agrawal, An, and Papagelis}}}
\bibcite{Aho:72}{{2}{1972}{{Aho and Ullman}}{{}}}
\bibcite{johnson2014}{{3}{2014--2019}{{et~al.}}{{}}}
\bibcite{APA:83}{{4}{1983}{{American Psychological Association}}{{}}}
\bibcite{Ando2005}{{5}{2005}{{Ando and Zhang}}{{}}}
\bibcite{andrew2007scalable}{{6}{2007}{{Andrew and Gao}}{{}}}
\bibcite{Chandra:81}{{7}{1981}{{Chandra et~al.}}{{Chandra, Kozen, and Stockmeyer}}}
\bibcite{eisner2016emoji2vec}{{8}{2016}{{Eisner et~al.}}{{Eisner, Rockt{\"a}schel, Augenstein, Bo{\v {s}}njak, and Riedel}}}
\bibcite{elrazzaz2017methodical}{{9}{2017}{{Elrazzaz et~al.}}{{Elrazzaz, Elbassuoni, Shaban, and Helwe}}}
\bibcite{Gusfield:97}{{10}{1997}{{Gusfield}}{{}}}
\bibcite{kunchukuttan2020indicnlp}{{11}{2020}{{Kunchukuttan}}{{}}}
\bibcite{mikolov2013efficient}{{12}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{outsios2019evaluation}{{13}{2019}{{Outsios et~al.}}{{Outsios, Karatsalos, Skianis, and Vazirgiannis}}}
\bibcite{rasooli-tetrault-2015}{{14}{2015}{{Rasooli and Tetreault}}{{}}}
\bibcite{sprugnoli2019vir}{{15}{2019}{{Sprugnoli et~al.}}{{Sprugnoli, Passarotti, and Moretti}}}
\newlabel{sed:acknowledgements}{{5}{7}{Acknowledgements}{section.5}{}}
\newlabel{sec:related}{{6}{7}{Related Work}{section.6}{}}
\newlabel{fig:biblicall}{{4}{8}{High performance on the Biblical Figures category indicates some level of biblical influence via the corpus. Interestingly, we see that greek embeddings optimized on the Modern Greek test set significantly outperformed the embeddings optimized for the Ancient Greek. This matches our intuition that things of a biblical nature have had a greater influence on Modern Greek than Ancient Greek.\relax }{figure.caption.5}{}}
\newlabel{fig:buddhism}{{5}{8}{Though many of the Indic language embeddings performed well on the Facets of Buddhism, surprisingly so did our Latin and Hebrew embeddings. This leads us to believe that some Buddhist concepts and words are shared by corpora spanning languages as diverse as Hebrew, Latin, and Hindi. At the same time, it should also be noted that Latin and Hebrew were two of the largest models trained compared to other classical languages and thus also likely benefit from greater resource richness.\relax }{figure.caption.6}{}}
\newlabel{fig:hinduism}{{6}{9}{Similar to Figure \ref {fig:biblical} we see languages more closely related to the topic of the category achieving better performance, in this case primarily the Indic languages. Interstingly, our Bengali embeddings performed poorly on this category suggesting that the corpus did not refer heavily to the Hindu context\relax }{figure.caption.7}{}}
